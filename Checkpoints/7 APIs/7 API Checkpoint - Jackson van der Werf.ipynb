{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import requests\r\n",
    "import pandas as pd\r\n",
    "import json\r\n",
    "\r\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAGnaTwEAAAAAhRdM6yLmei6skyaWcjbx8IDFnlw%3DLPQHO2CTw1nVjjHLx3htgP9qmeCOgPpt96EdDujokNcWljI5iP'\r\n",
    "headers = {'Authorization':('Bearer '+ bearer_token)}\r\n",
    "\r\n",
    "n = 100                            # The total number of tweets we want\r\n",
    "max_results = 10                  # The number of tweets to pull per request; must be between 10 and 100\r\n",
    "total_retrieved = 0               # To keep track of when to stop\r\n",
    "next_token = \"\"                   # Must be empty on first iteration\r\n",
    "search_term = \"covid\"    # To form an advanced query, see here: https://twitter.com/search-advanced?lang=en\r\n",
    "since_id = \"1371600000000000000\"  # The id of the oldest tweet you want to retrieve\r\n",
    "df = pd.DataFrame(columns=['Retweets', 'Likes', 'Text'], index=['ID'])\r\n",
    "\r\n",
    "# stop when we have n results\r\n",
    "while total_retrieved < n:\r\n",
    "\r\n",
    "  # the first time through the loop, we do not need the next_token parameter\r\n",
    "  if next_token == \"\":\r\n",
    "    # url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&since_id={since_id}'\r\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}'\r\n",
    "  else:\r\n",
    "    # url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&since_id={since_id}&next_token={next_token}'\r\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&next_token={next_token}'\r\n",
    "\r\n",
    "  # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\r\n",
    "  url += f'&user.fields=created_at,description,entities,id,location,name,profile_image_url,protected,public_metrics,url,username,verified,withheld'\r\n",
    "  url += f'&tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,referenced_tweets,reply_settings,source,text,withheld'\r\n",
    "  url += f'&expansions=attachments.poll_ids,attachments.media_keys,author_id,geo.place_id,in_reply_to_user_id,referenced_tweets.id,entities.mentions.username,referenced_tweets.id.author_id'\r\n",
    "  url += f'&media.fields=duration_ms,height,media_key,preview_image_url,public_metrics,type,url,width'\r\n",
    "  url += f'&place.fields=contained_within,country,country_code,full_name,geo,id,name,place_type'\r\n",
    "  url += f'&poll.fields=duration_minutes,end_datetime,id,options,voting_status'\r\n",
    "\r\n",
    "  # make the request to the Twitter API Recent Search endpoint\r\n",
    "  response = requests.request(\"GET\", url, headers=headers)\r\n",
    "  json_data = json.loads(response.text)\r\n",
    "\r\n",
    "  # Create the empty DataFrame with the columns you want\r\n",
    "  df = pd.DataFrame(columns=['id', 'retweets', 'likes', 'url', 'text'])\r\n",
    "  df.set_index('id', inplace=True)\r\n",
    "\r\n",
    "  for tweet in json_data['data']:\r\n",
    "    media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\r\n",
    "\r\n",
    "    # Store the data into variables\r\n",
    "    tweet_id = tweet['id']\r\n",
    "    retweet_count = tweet['public_metrics']['retweet_count']\r\n",
    "    like_count = tweet['public_metrics']['like_count']\r\n",
    "    image_url = \"\"\r\n",
    "    text = tweet['text']\r\n",
    "\r\n",
    "    # Find out if there is media\r\n",
    "    if 'attachments' in tweet:\r\n",
    "      if 'media_keys' in tweet['attachments']:\r\n",
    "        media_key = tweet['attachments']['media_keys'][0]\r\n",
    "\r\n",
    "    # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\r\n",
    "    if media_key != \"\":\r\n",
    "      for media in json_data['includes']['media']:\r\n",
    "        if media['media_key'] == media_key: # Only if the media_key matches the one we stored\r\n",
    "          if media['type'] == 'photo':      # Only if it is a photo; ignore videos\r\n",
    "            image_url = media['url']        # Store the url in a variable\r\n",
    "\r\n",
    "    # Add the new data to a new record in the DataFrame\r\n",
    "    df.loc[tweet_id] = [retweet_count, like_count, image_url, text]\r\n",
    "\r\n",
    "  # keep track of how many results have been obtained so far:\r\n",
    "  # # total_retrieved += int(json_data['meta']['result_count'])\r\n",
    "  total_retrieved += 1\r\n",
    "  # try:\r\n",
    "  #   next_token = json_data['next']['next_token']\r\n",
    "  # except:\r\n",
    "  #   print(\"No more results\")\r\n",
    "\r\n",
    "    # keep track of where to start next time, but quit if there are no more results\r\n",
    "  try:\r\n",
    "    next_token = json_data['meta']['next_token']\r\n",
    "  except:\r\n",
    "    break  \r\n",
    "\r\n",
    "print(f'Number of records:\\t{len(df)}')\r\n",
    "df.head()\r\n",
    "\r\n",
    "  "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of records:\t10\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1441114616591904770</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>RT @CarmenMForman: After the OKC Thunder annou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441114616495411202</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>@Nate_Tice @TheFalcoholic Did anyone else noti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441114616461869057</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>RT @tucamaiacaetano: - A chance da Terra ser a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441114616185032709</th>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>RT @JamesMelville: As a consequence of the Cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441114616017219586</th>\n",
       "      <td>929</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>RT @NielsErikJ: Today I had to tell 6 people t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    retweets likes url  \\\n",
       "id                                       \n",
       "1441114616591904770       50     0       \n",
       "1441114616495411202        0     1       \n",
       "1441114616461869057       12     0       \n",
       "1441114616185032709      289     0       \n",
       "1441114616017219586      929     0       \n",
       "\n",
       "                                                                  text  \n",
       "id                                                                      \n",
       "1441114616591904770  RT @CarmenMForman: After the OKC Thunder annou...  \n",
       "1441114616495411202  @Nate_Tice @TheFalcoholic Did anyone else noti...  \n",
       "1441114616461869057  RT @tucamaiacaetano: - A chance da Terra ser a...  \n",
       "1441114616185032709  RT @JamesMelville: As a consequence of the Cov...  \n",
       "1441114616017219586  RT @NielsErikJ: Today I had to tell 6 people t...  "
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# In this example, only those tweets with photos/images are stored\r\n",
    "\r\n",
    "n = 200                           # The total number of tweets we want\r\n",
    "max_results = 100                 # The number of tweets to pull per request; must be between 10 and 100\r\n",
    "total_retrieved = 0               # To keep track of when to stop\r\n",
    "next_token = \"\"                   # Must be empty on first iteration\r\n",
    "search_term = \"canyoneering\"             # To form an advanced query, see here: https://twitter.com/search-advanced?lang=en\r\n",
    "since_id = \"1371590000000000000\"  # The id of the oldest tweet you want to retrieve\r\n",
    "\r\n",
    "# Create the empty DataFrame with the columns you want\r\n",
    "df_img = pd.DataFrame(columns=['id', 'retweets', 'likes', 'url', 'text'])\r\n",
    "df_img.set_index('id', inplace=True)\r\n",
    "\r\n",
    "# stop when we have n results\r\n",
    "while total_retrieved < n:\r\n",
    "\r\n",
    "  # the first time through the loop, we do not need the next_token parameter\r\n",
    "  if next_token == \"\":\r\n",
    "    # url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&since_id={since_id}'\r\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}'\r\n",
    "  else:\r\n",
    "    # url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&since_id={since_id}&next_token={next_token}'\r\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&next_token={next_token}'\r\n",
    "\r\n",
    "  # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\r\n",
    "  url += f'&tweet.fields=attachments,public_metrics,text'\r\n",
    "  url += f'&expansions=attachments.media_keys'\r\n",
    "  url += f'&media.fields=media_key,type,url'\r\n",
    "\r\n",
    "  # make the request to the Twitter API Recent Search endpoint\r\n",
    "  response = requests.request(\"GET\", url, headers=headers)\r\n",
    "  try:  # Just in case we get an error\r\n",
    "    json_data = json.loads(response.text)\r\n",
    "  except:\r\n",
    "    print(response.text)\r\n",
    "  \r\n",
    "\r\n",
    "  for tweet in json_data['data']:\r\n",
    "    media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\r\n",
    "\r\n",
    "    # Store the data into variables\r\n",
    "    tweet_id = tweet['id']\r\n",
    "    retweet_count = tweet['public_metrics']['retweet_count']\r\n",
    "    like_count = tweet['public_metrics']['like_count']\r\n",
    "    image_url = \"\"\r\n",
    "    text = tweet['text']\r\n",
    "\r\n",
    "    # Find out if there is media\r\n",
    "    if 'attachments' in tweet:\r\n",
    "      if 'media_keys' in tweet['attachments']:\r\n",
    "        media_key = tweet['attachments']['media_keys'][0]\r\n",
    "\r\n",
    "    # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\r\n",
    "    if media_key != \"\":\r\n",
    "      for media in json_data['includes']['media']:\r\n",
    "        if media['media_key'] == media_key: # Only if the media_key matches the one we stored\r\n",
    "          if media['type'] == 'photo':      # Only if it is a photo; ignore videos\r\n",
    "            image_url = media['url']        # Store the url in a variable\r\n",
    "            \r\n",
    "            # Only iterate if a photo is found\r\n",
    "            total_retrieved += 1\r\n",
    "            \r\n",
    "            # Only add the record in the DataFrame if a photo is found\r\n",
    "            df_img.loc[tweet_id] = [retweet_count, like_count, image_url, text]\r\n",
    "            break\r\n",
    "\r\n",
    "  # keep track of where to start next time, but quit if there are no more results\r\n",
    "  try:\r\n",
    "    next_token = json_data['meta']['next_token']\r\n",
    "  except:\r\n",
    "    break  \r\n",
    "\r\n",
    "print(f'Number of records:\\t{len(df_img)}')\r\n",
    "# df_img.to_csv('twitter.csv')\r\n",
    "df_img.head()\r\n",
    "\r\n",
    "  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# In this example, only those tweets with photos/images are stored\r\n",
    "\r\n",
    "n = 200                           # The total number of tweets we want\r\n",
    "max_results = 100                 # The number of tweets to pull per request; must be between 10 and 100\r\n",
    "total_retrieved = 0               # To keep track of when to stop\r\n",
    "next_token = \"\"                   # Must be empty on first iteration\r\n",
    "search_term = \"canyoneering\"             # To form an advanced query, see here: https://twitter.com/search-advanced?lang=en\r\n",
    "since_id = \"1371590000000000000\"  # The id of the oldest tweet you want to retrieve\r\n",
    "\r\n",
    "# Create the empty DataFrame with the columns you want\r\n",
    "df_img = pd.DataFrame(columns=['id', 'retweets', 'likes', 'text'])\r\n",
    "df_img.set_index('id', inplace=True)\r\n",
    "\r\n",
    "# stop when we have n results\r\n",
    "while total_retrieved < n:\r\n",
    "  print(total_retrieved)\r\n",
    "\r\n",
    "  # the first time through the loop, we do not need the next_token parameter\r\n",
    "  if next_token == \"\":\r\n",
    "    # url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&since_id={since_id}'\r\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}'\r\n",
    "  else:\r\n",
    "    # url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&since_id={since_id}&next_token={next_token}'\r\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&next_token={next_token}'\r\n",
    "\r\n",
    "  # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\r\n",
    "  url += f'&tweet.fields=attachments,public_metrics,text'\r\n",
    "  url += f'&expansions=attachments.media_keys'\r\n",
    "  url += f'&media.fields=media_key,type,url'\r\n",
    "\r\n",
    "  # make the request to the Twitter API Recent Search endpoint\r\n",
    "  response = requests.request(\"GET\", url, headers=headers)\r\n",
    "  try:  # Just in case we get an error\r\n",
    "    json_data = json.loads(response.text)\r\n",
    "  except:\r\n",
    "    print(response.text)\r\n",
    "  \r\n",
    "\r\n",
    "  for tweet in json_data['data']:\r\n",
    "    media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\r\n",
    "\r\n",
    "    # Store the data into variables\r\n",
    "    tweet_id = tweet['id']\r\n",
    "    retweet_count = tweet['public_metrics']['retweet_count']\r\n",
    "    like_count = tweet['public_metrics']['like_count']\r\n",
    "    image_url = \"\"\r\n",
    "    text = tweet['text']\r\n",
    "\r\n",
    "    # Find out if there is media\r\n",
    "    if 'attachments' in tweet:\r\n",
    "      if 'media_keys' in tweet['attachments']:\r\n",
    "        media_key = tweet['attachments']['media_keys'][0]\r\n",
    "\r\n",
    "    # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\r\n",
    "    if media_key != \"\":\r\n",
    "      for media in json_data['includes']['media']:\r\n",
    "        if media['media_key'] == media_key: # Only if the media_key matches the one we stored\r\n",
    "          if media['type'] == 'photo':      # Only if it is a photo; ignore videos\r\n",
    "            image_url = media['url']        # Store the url in a variable\r\n",
    "            \r\n",
    "            \r\n",
    "\r\n",
    "  df_img.loc[tweet_id] = [retweet_count, like_count, text]\r\n",
    "  # keep track of where to start next time, but quit if there are no more results\r\n",
    "  try:\r\n",
    "    next_token = json_data['meta']['next_token']\r\n",
    "    # Only iterate if a photo is found\r\n",
    "    total_retrieved += 1\r\n",
    "    # Only add the record in the DataFrame if a photo is found\r\n",
    "    break\r\n",
    "  except:\r\n",
    "    break  \r\n",
    "\r\n",
    "print(f'Number of records:\\t{len(df_img)}')\r\n",
    "# df_img.to_csv('twitter.csv')\r\n",
    "df_img.head()\r\n",
    "\r\n",
    "  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_img.shape"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "67645e4edde45144644326d4815cd26b14c8093db79836f9b69d4c3e18d9122c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}