{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Retrieval\n",
    "Data Collection and Retrieval (same requirements as the individual data retrieval project):\n",
    "* Topic: anything you choose, but it must include the following data types:\n",
    "    * numeric features\n",
    "    * categorical features (to be dummy coded)\n",
    "    * text features (to be processed using text analytics)\n",
    "    * image features (to be processed using image classification) \n",
    "    * labels to choose from (i.e. outcomes that you want to predict with the other features)\n",
    "* Scrape approximately ~500 records; +/- 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis\n",
    "!pip install pyLDAvis.gensim\n",
    "!pip install logging\n",
    "!pip install nltk\n",
    "!pip install - U pip setuptools wheel\n",
    "!pip install - U spacy\n",
    "!python - m spacy download en_core_web_sm\n",
    "!pip install gower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_tweets(bearer_token):\n",
    "    headers = {'Authorization': ('Bearer ' + bearer_token)}\n",
    "\n",
    "    n = 525                               # The total number of tweets we want\n",
    "    max_results = 10 # The number of tweets to pull per request; must be between 10 and 100\n",
    "    total_retrieved = 0                   # To keep track of when to stop\n",
    "    next_token = \"\"                       # Must be empty on first iteration\n",
    "    search_term = \"manchester%20united\"\n",
    "\n",
    "    # Create empty DataFrames and set columns\n",
    "    df_tweets = pd.DataFrame(columns=['tweet_id', 'author_id', 'retweet_count', 'like_count',\n",
    "                                      'text', 'language', 'created_at', 'source', 'possibly_sensitive', 'image_url'])\n",
    "\n",
    "    # stop when we have n results\n",
    "    while total_retrieved < n:\n",
    "\n",
    "        # the first time through the loop, we do not need the next_token parameter\n",
    "        if next_token == \"\":\n",
    "            url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}'\n",
    "        else:\n",
    "            url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&next_token={next_token}'\n",
    "\n",
    "        # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\n",
    "        url += f'&expansions=geo.place_id,author_id,attachments.media_keys'\n",
    "        url += f'&tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,referenced_tweets,reply_settings,source,text,withheld'\n",
    "        url += f'&media.fields=media_key,type,url&user.fields=created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld'\n",
    "\n",
    "        # make the request to the Twitter API Recent Search endpoint\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "        try:  # Just in case we get an error\n",
    "            json_data = json.loads(response.text)\n",
    "            # print(json_data)\n",
    "        except:\n",
    "            print(response.text)\n",
    "\n",
    "        for tweet in json_data['data']:\n",
    "            media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\n",
    "\n",
    "            # Store the data into variables\n",
    "            tweet_id = tweet['id']\n",
    "            author_id = tweet['author_id']\n",
    "            retweet_count = tweet['public_metrics']['retweet_count']    # label\n",
    "            like_count = tweet['public_metrics']['like_count']          # label\n",
    "            image_url = \"\"                                              # image\n",
    "            text = tweet['text']                                        # text\n",
    "            created_at = tweet['created_at']                            # categorical\n",
    "            source = tweet['source']                                    # categorical\n",
    "            possibly_sensitive = tweet['possibly_sensitive']            # categorical\n",
    "            language = tweet['lang']                                    # categorical\n",
    "\n",
    "            # Find out if there is media\n",
    "            if 'attachments' in tweet:\n",
    "                if 'media_keys' in tweet['attachments']:\n",
    "                    media_key = tweet['attachments']['media_keys'][0]\n",
    "\n",
    "            # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\n",
    "            if media_key != \"\":\n",
    "                for media in json_data['includes']['media']:\n",
    "                    # Only if the media_key matches the one we stored\n",
    "                    if media['media_key'] == media_key:\n",
    "                        if media['type'] == 'photo':      # Only if it is a photo; ignore videos\n",
    "                            # Store the url in a variable\n",
    "                            image_url = media['url']\n",
    "\n",
    "            # Add the new data to a new record in the DataFrame\n",
    "            df_tweets.loc[tweet_id] = [tweet_id, author_id, retweet_count, like_count,\n",
    "                                       text, language, created_at, source, possibly_sensitive, image_url]\n",
    "\n",
    "        # keep track of how many results have been obtained so far:\n",
    "        total_retrieved += 10\n",
    "        print(f'{total_retrieved} tweets retrieved')\n",
    "\n",
    "        # keep track of where to start next time, but quit if there are no more results\n",
    "        try:\n",
    "            next_token = json_data['meta']['next_token']\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        # sleep to avoid hitting the rate limit\n",
    "        time.sleep(8)\n",
    "\n",
    "    # All done! save the dataframes to csv files\n",
    "    df_tweets.to_csv('tweets.csv')\n",
    "    print('Got all the tweets!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_tweets(bearer_token='AAAAAAAAAAAAAAAAAAAAAGnaTwEAAAAAhRdM6yLmei6skyaWcjbx8IDFnlw%3DLPQHO2CTw1nVjjHLx3htgP9qmeCOgPpt96EdDujokNcWljI5iP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Cleaning and Engineering\n",
    "* The cleaning steps should include any basic steps needed to prepare the data including:\n",
    "    * Binning rare group values\n",
    "    * Standardizing values\n",
    "    * Adjusting for skewness\n",
    "    * Handle missing values\n",
    "* These steps will be unique to each dataset\n",
    "* Engineering includes converting the unstructured text and images features into usable features. This could include:\n",
    "    * Topic moding text\n",
    "    * Extracting characteristics of the text (e.g. word counts, sentiment)\n",
    "    * Extracting characteristics of the images (e..g number of faces, smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin rare group values\n",
    "def bin_groups(df, percent=.05, cols_to_exclude=[]):\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if col not in cols_to_exclude:\n",
    "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "                for group, count in df[col].value_counts().iteritems():\n",
    "                    if count / len(df) < percent:\n",
    "                        df.loc[df[col] == group, col] = 'Other'\n",
    "    return df\n",
    "\n",
    "# Standardize data\n",
    "\n",
    "\n",
    "def standardize_values(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    # Scale/normalize the features\n",
    "    df = pd.DataFrame(preprocessing.MinMaxScaler(\n",
    "    ).fit_transform(df), columns=df.columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Dummy code categorical variables\n",
    "\n",
    "\n",
    "def dummy_code_categorical_variables(df):\n",
    "    import pandas as pd\n",
    "\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = df.join(pd.get_dummies(\n",
    "                df[col], prefix=col, drop_first=True, lsuffix='_left', rsuffix='_right'))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "\n",
    "\n",
    "def drop_columns_missing_data(df, cutoff=.5):\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if df[col].isna().sum() / len(df) > cutoff:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "\n",
    "\n",
    "def impute_mean(df):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "# Handle missing values AND standardize values\n",
    "\n",
    "\n",
    "def impute_KNN(df):\n",
    "    from sklearn.impute import KNNImputer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns=df.columns)\n",
    "    imp = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "# Handle missing values\n",
    "\n",
    "\n",
    "def impute_reg(df):\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    imp = IterativeImputer(max_iter=10, random_state=12345)\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fit_mlr(df, test_size=.2, random_state=12345, label=''):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    X = df.drop(label, axis=1)\n",
    "    y = df[label]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state)\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    print(f'R-squared (mlr): \\t{model.score(X_test, y_test)}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    import pandas as pd\n",
    "    from numpy import mean, std\n",
    "    X = df.drop(label, axis=1)\n",
    "    y = df[label]\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=12345)\n",
    "    else:\n",
    "        cv = KFold(n_splits=10, random_state=12345, shuffle=True)\n",
    "    scores = cross_val_score(LinearRegression(), X, y,\n",
    "                             scoring='r2', cv=cv, n_jobs=-1)\n",
    "    print(f'Average R-squared:\\t{mean(scores)}')\n",
    "    return LinearRegression().fit(X, y)\n",
    "\n",
    "\n",
    "def calc_sentiment(df):\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "    nltk.download('vader_lexicon')\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df['sentiment_overall'] = 0.0\n",
    "    df['sentiment_negative'] = 0.0\n",
    "    df['sentiment_neutral'] = 0.0\n",
    "    df['sentiment_positive'] = 0.0\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        sentiment = sia.polarity_scores(row.text)\n",
    "        df.at[row.Index, 'sentiment_overall'] = sentiment['compound']\n",
    "        df.at[row.Index, 'sentiment_negative'] = sentiment['neg']\n",
    "        df.at[row.Index, 'sentiment_neutral'] = sentiment['neu']\n",
    "        df.at[row.Index, 'sentiment_positive'] = sentiment['pos']\n",
    "\n",
    "    # df.drop(columns=['Sentiment'], inplace=True) # I don't think this is necessary\n",
    "    return df\n",
    "\n",
    "\n",
    "def image_classification(df, api_key, api_secret):\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import json\n",
    "    df_imagga = pd.DataFrame(columns=[\"interior objects\", \"nature landscape\", \"beaches seaside\", \"events parties\", \"food drinks\",\n",
    "                                      \"paintings art\", \"pets animals\", \"text visuals\", \"sunrises sunsets\", \"cars vehicles\",\n",
    "                                      \"macro flowers\", \"streetview architecture\", \"people portraits\"])\n",
    "    for row in df.itertuples():\n",
    "        tweet_id = row.tweet_id\n",
    "        if pd.isnull(row.image_url) or pd.isna(row.image_url):\n",
    "            scores = [0.0] * len(df_imagga.columns)\n",
    "\n",
    "            for n, col in enumerate(df_imagga.columns):\n",
    "                # Iterate through each category of the result\n",
    "                scores[n] = 0.0\n",
    "                # Store the list as a new row in the DataFrame\n",
    "                df_imagga.loc[tweet_id] = scores\n",
    "        else:\n",
    "            url = 'https://api.imagga.com/v2/categories/personal_photos/?image_url=' + row.image_url\n",
    "            request = requests.get(url, auth=(api_key, api_secret))\n",
    "            json_data = json.loads(request.text)\n",
    "\n",
    "            # Create a list of 0.0 scores to update as we get data for each category we want to score in our DataFrame\n",
    "            scores = [0.0] * len(df_imagga.columns)\n",
    "\n",
    "            # Find the associated column in the DataFrame\n",
    "            for n, col in enumerate(df_imagga.columns):\n",
    "                # Iterate through each category of the result\n",
    "                for category in json_data[\"result\"][\"categories\"]:\n",
    "                    if col == category['name']['en']:\n",
    "                        # Store the score\n",
    "                        scores[n] = category['confidence']\n",
    "                        break  # No need to keep looping once we've found the score\n",
    "                # Store the list as a new row in the DataFrame\n",
    "                df_imagga.loc[tweet_id] = scores\n",
    "\n",
    "    # merge the two DataFrames\n",
    "    df = pd.merge(df, df_imagga, left_on=df.tweet_id, right_on=df_imagga.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_random_columns(df):\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    if 'key_0' in df.columns:\n",
    "        df.drop(columns=['key_0'], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def correct_skew(df):\n",
    "    import numpy as np\n",
    "    for col in df:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            print(col + \" Normal: \" + str(df[col].skew()))\n",
    "            if (df[col].skew() > 1) or (df[col].skew() < -1):\n",
    "                log = np.log(df[col] + 1).skew()\n",
    "                print(log)\n",
    "                cbrt = (df[col]**(1/3)).skew()\n",
    "                print(cbrt)\n",
    "                sqrt = (df[col]**(1/2)).skew()\n",
    "                print(sqrt)\n",
    "                lowest = min(abs(log), abs(cbrt), abs(sqrt))\n",
    "                print(\"Lowest: \" + str(lowest))\n",
    "                if abs(log) == lowest:\n",
    "                    df[col] = np.log(df[col] + 1)\n",
    "                elif abs(cbrt) == lowest:\n",
    "                    df[col] = df[col]**(1/3)\n",
    "                elif abs(sqrt) == lowest:\n",
    "                    df[col] == df[col]**(1/2)\n",
    "                print(col + \" Fixed :\" + str(df[col].skew()))\n",
    "    for col in df:\n",
    "        if pd.api.types.is_numeric_dtype(col):\n",
    "            print(col + \"Final: \" + str(df[col].skew()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def vif(df):\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # initialize dictionaries\n",
    "    vif_dict, tolerance_dict = {}, {}\n",
    "\n",
    "    # drop the following columns with high VIF scores (check manually each time :))\n",
    "    for col in df.drop(columns=['key_0', 'tweet_id', 'author_id', 'retweet_count', 'sentiment_neutral', 'sentiment_positive', 'sentiment_negative',\n",
    "                                'cars vehicles', 'sunrises sunsets', 'macro flowers', 'text visuals', 'streetview architecture', 'pets animals',\n",
    "                                'interior objects', 'nature landscape', 'beaches seaside', 'events parties', 'food drinks', 'paintings art', 'people portraits']):\n",
    "\n",
    "        y = df[col]\n",
    "        X = df.drop(columns=[col])\n",
    "\n",
    "        # extract r-squared from the fit\n",
    "        r_squared = LinearRegression().fit(X, y).score(X, y)\n",
    "\n",
    "        # calculate VIF\n",
    "        if r_squared < 1:  # Prevent division by zero runtime error\n",
    "            vif = 1/(1 - r_squared)\n",
    "        else:\n",
    "            vif = 100\n",
    "\n",
    "    df = df.drop(columns=['key_0', 'tweet_id', 'author_id', 'retweet_count', 'sentiment_neutral', 'sentiment_positive', 'sentiment_negative',\n",
    "                              'cars vehicles', 'sunrises sunsets', 'macro flowers', 'text visuals', 'streetview architecture', 'pets animals',\n",
    "                              'interior objects', 'nature landscape', 'beaches seaside', 'events parties', 'food drinks', 'paintings art', 'people portraits'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_0 Normal: -0.00645078690584609\n",
      "tweet_id Normal: -0.00645078690584609\n",
      "author_id Normal: -0.03308385656702665\n",
      "retweet_count Normal: 4.028599276044782\n",
      "0.19855164334438574\n",
      "1.5934290240276734\n",
      "2.452823685565728\n",
      "Lowest: 0.19855164334438574\n",
      "retweet_count Fixed :0.19855164334438574\n",
      "like_count Normal: 22.408839487242897\n",
      "7.146213399013339\n",
      "8.678429344032534\n",
      "14.89442703538303\n",
      "Lowest: 7.146213399013339\n",
      "like_count Fixed :7.146213399013339\n",
      "possibly_sensitive Normal: 7.092516463333769\n",
      "7.092516463333769\n",
      "7.092516463333769\n",
      "7.092516463333769\n",
      "Lowest: 7.092516463333769\n",
      "possibly_sensitive Fixed :7.092516463333769\n",
      "sentiment_overall Normal: -0.5316425643190593\n",
      "sentiment_negative Normal: 0.7501774619485463\n",
      "sentiment_neutral Normal: -0.43866285408577016\n",
      "sentiment_positive Normal: 0.3381528858298353\n",
      "interior objects Normal: 0\n",
      "nature landscape Normal: 0\n",
      "beaches seaside Normal: 0\n",
      "events parties Normal: 16.257739041956057\n",
      "16.233691353286268\n",
      "16.235317904867596\n",
      "16.238827741047768\n",
      "Lowest: 16.233691353286268\n",
      "events parties Fixed :16.233691353286268\n",
      "food drinks Normal: 23.021728866442672\n",
      "23.021728866442686\n",
      "23.021728866442672\n",
      "23.021728866442672\n",
      "Lowest: 23.021728866442672\n",
      "food drinks Fixed :23.021728866442672\n",
      "paintings art Normal: 0\n",
      "pets animals Normal: 0\n",
      "text visuals Normal: 23.02172886644267\n",
      "23.021728866442675\n",
      "23.02172886644267\n",
      "23.021728866442672\n",
      "Lowest: 23.02172886644267\n",
      "text visuals Fixed :23.02172886644267\n",
      "sunrises sunsets Normal: 0\n",
      "cars vehicles Normal: 0\n",
      "macro flowers Normal: 0\n",
      "streetview architecture Normal: 0\n",
      "people portraits Normal: 23.02172886644266\n",
      "23.02172886644268\n",
      "23.02172886644267\n",
      "23.02172886644269\n",
      "Lowest: 23.02172886644267\n",
      "people portraits Fixed :23.02172886644267\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>sentiment_overall</th>\n",
       "      <th>language_en</th>\n",
       "      <th>language_es</th>\n",
       "      <th>language_in</th>\n",
       "      <th>language_pt</th>\n",
       "      <th>source_Twitter Web App</th>\n",
       "      <th>source_Twitter for Android</th>\n",
       "      <th>source_Twitter for iPhone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283954</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>0.175402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.688413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>530 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     like_count  possibly_sensitive  sentiment_overall  language_en  \\\n",
       "0      0.000000                 0.0           0.283954          1.0   \n",
       "1      0.000000                 0.0           0.664719          0.0   \n",
       "2      0.000000                 0.0           0.664719          1.0   \n",
       "3      0.000000                 0.0           1.000000          1.0   \n",
       "4      0.000000                 0.0           0.555774          1.0   \n",
       "..          ...                 ...                ...          ...   \n",
       "525    0.000000                 0.0           0.664719          0.0   \n",
       "526    0.000000                 0.0           0.699784          1.0   \n",
       "527    0.175402                 0.0           0.688413          0.0   \n",
       "528    0.000000                 0.0           0.555774          1.0   \n",
       "529    0.000000                 0.0           0.664719          0.0   \n",
       "\n",
       "     language_es  language_in  language_pt  source_Twitter Web App  \\\n",
       "0            0.0          0.0          0.0                     0.0   \n",
       "1            1.0          0.0          0.0                     0.0   \n",
       "2            0.0          0.0          0.0                     0.0   \n",
       "3            0.0          0.0          0.0                     0.0   \n",
       "4            0.0          0.0          0.0                     1.0   \n",
       "..           ...          ...          ...                     ...   \n",
       "525          0.0          0.0          1.0                     0.0   \n",
       "526          0.0          0.0          0.0                     0.0   \n",
       "527          1.0          0.0          0.0                     0.0   \n",
       "528          0.0          0.0          0.0                     1.0   \n",
       "529          0.0          1.0          0.0                     0.0   \n",
       "\n",
       "     source_Twitter for Android  source_Twitter for iPhone  \n",
       "0                           1.0                        0.0  \n",
       "1                           1.0                        0.0  \n",
       "2                           0.0                        1.0  \n",
       "3                           0.0                        1.0  \n",
       "4                           0.0                        0.0  \n",
       "..                          ...                        ...  \n",
       "525                         1.0                        0.0  \n",
       "526                         0.0                        0.0  \n",
       "527                         1.0                        0.0  \n",
       "528                         0.0                        0.0  \n",
       "529                         0.0                        1.0  \n",
       "\n",
       "[530 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data cleaning and Prep\n",
    "df = bin_groups(df, percent=.05, cols_to_exclude=['text', 'created_at', 'image_url']) # Exclude columns it doesn't make sense to bin\n",
    "\n",
    "# Engineering\n",
    "df = calc_sentiment(df)\n",
    "df = image_classification(df=df, api_key='acc_81f8f349aa51d69', api_secret='dd79216824834fc2b208323818d1ca35')\n",
    "\n",
    "# drop columns I'm finished analyzing\n",
    "df = df.drop(columns=['text', 'created_at', 'image_url'])\n",
    "\n",
    "# INSERT SKEW FUNCTION HERE\n",
    "df = correct_skew(df)\n",
    "\n",
    "# impute KNN, also standardize values\n",
    "df = impute_KNN(df)\n",
    "\n",
    "# INSERT VIF FUNCTION HERE\n",
    "df = vif(df)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling \n",
    "(same requirements of the modeling project)\n",
    "* Generate the best possible model for:\n",
    "    * Regression\n",
    "    * Classification\n",
    "    * Clustering\n",
    "* You choose which features to keep in the model model\n",
    "* There is no required level of fit metric. Your task is simply to get the best fit metrics possible--not achieve a certain value. All datasets are different and there is no way to compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_selection(df, cols_to_drop=[]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import model_selection\n",
    "    from sklearn import preprocessing\n",
    "    import sklearn.neural_network as nn\n",
    "    from sklearn.linear_model import RidgeCV, LassoCV\n",
    "    import sklearn.ensemble as se\n",
    "    import sklearn.tree as tree\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "    from sklearn import gaussian_process\n",
    "    from sklearn import neighbors\n",
    "    from sklearn import svm\n",
    "    import sklearn.linear_model as lm\n",
    "    import pickle\n",
    "\n",
    "    df = df.select_dtypes(np.number)  # Remove categorical features first\n",
    "    y = df.like_count                    # Save the label first\n",
    "    # Remove the label from the feature list\n",
    "    X = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Scale/normalize the features\n",
    "    X = pd.DataFrame(preprocessing.MinMaxScaler(\n",
    "    ).fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "    # Eyeball the data to make sure it looks right:\n",
    "    X_train\n",
    "\n",
    "    fit = {}  # Use this to store each of the fit metrics\n",
    "\n",
    "    # 1. LINEAR MODELS: assumes normal distribution, homoscedasticity, no multi-collinearity, independence, and no auto-correlation (some exceptions apply)\n",
    "\n",
    "    # 1.1. Ordinary Least Squares Multiple Linear Regression\n",
    "    model_ols = lm.LinearRegression()\n",
    "    model_ols.fit(X_train, y_train)\n",
    "    fit['OrdinaryLS R'] = model_ols.score(X_test, y_test)\n",
    "\n",
    "    # 1.2. Ridge Regression: more robust to multi-collinearity\n",
    "    # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_rr = lm.Ridge(alpha=0.5)\n",
    "    model_rr.fit(X_train, y_train)\n",
    "    fit['Ridge R'] = model_rr.score(X_test, y_test)\n",
    "\n",
    "    # 1.3. Lasso Regression: better for sparse values like RetweetCount where most are zeros but a few have many retweets.\n",
    "    # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_lr = lm.Lasso(alpha=0.1)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    fit['Lasso R'] = model_lr.score(X_test, y_test)\n",
    "\n",
    "    # 1.4. Least Angle Regression: good when the number of features is greater than the number of samples\n",
    "    # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_llr = lm.LassoLars(alpha=0.1)\n",
    "    model_llr.fit(X_train, y_train)\n",
    "    fit['LARS Lasso R'] = model_llr.score(X_test, y_test)\n",
    "\n",
    "    # 1.5. Bayesian Regression: probability based; allows regularization parameters, automatically tuned to data\n",
    "    model_br = lm.BayesianRidge()\n",
    "    model_br.fit(X_train, y_train)\n",
    "    fit['Bayesian R'] = model_br.score(X_test, y_test)\n",
    "\n",
    "    # SUPPORT VECTOR MACHINES\n",
    "    # 1.9. SVM: this is the default SVM, parameters can be modified to make this more accurate\n",
    "    model_svm = svm.SVR()\n",
    "    model_svm.fit(X_train, y_train)\n",
    "    fit['SupportVM R'] = model_svm.score(X_test, y_test)\n",
    "\n",
    "    # 1.10. Linear SVM: Faster than SVM but only considers a linear model\n",
    "    model_lsvm = svm.LinearSVR()\n",
    "    model_lsvm.fit(X_train, y_train)\n",
    "    fit['Linear SVM R'] = model_lsvm.score(X_test, y_test)\n",
    "\n",
    "    # 1.11. NuSVM:\n",
    "    model_nusvm = svm.NuSVR()\n",
    "    model_nusvm.fit(X_train, y_train)\n",
    "    fit['NuSupportVM R'] = model_nusvm.score(X_test, y_test)\n",
    "\n",
    "    # STOCHASTIC GRADIENT DESCENT REGRESSION\n",
    "    # 1.12. SGDRegressor:\n",
    "    model_sgdr = lm.SGDRegressor()\n",
    "    model_sgdr.fit(X_train, y_train)\n",
    "    fit['SGradientD R'] = model_sgdr.score(X_test, y_test)\n",
    "\n",
    "    # KNN: NEAREST NEIGHBORS REGRESSION\n",
    "\n",
    "    # 1.13. KNeighborsRegressor:\n",
    "    # model_knnr = neighbors.KNeighborsRegressor(5, 'uniform')\n",
    "    # model_knnr.fit(X_train, y_train)\n",
    "    # fit['KNNeighbors R'] = model_knnr.score(X_test, y_test)\n",
    "\n",
    "    # 1.14. KNeighborsRegressor:\n",
    "    # model_knnrd = neighbors.KNeighborsRegressor(8, 'distance')\n",
    "    # model_knnrd.fit(X_train, y_train)\n",
    "    # fit['KNNeighborsD R'] = model_knnrd.score(X_test, y_test)\n",
    "\n",
    "    # GAUSSIAN PROCESS REGRESSION\n",
    "\n",
    "    # 1.15. GaussianProcessRegressor:\n",
    "    model_gpr = gaussian_process.GaussianProcessRegressor(\n",
    "        DotProduct() + WhiteKernel())\n",
    "    model_gpr.fit(X_train, y_train)\n",
    "    fit['GaussianP R'] = model_gpr.score(X_test, y_test)\n",
    "\n",
    "    # Sort and print the dictionary by greatest R squared to least\n",
    "    r2s = sorted_list_by_value = sorted(fit, key=fit.__getitem__, reverse=True)\n",
    "    for r2 in r2s:\n",
    "        print(f'{r2}:\\t{fit[r2]}')\n",
    "\n",
    "    # Select the model with the highest R squared\n",
    "    print(f'Best Regression Model: {r2s[0]} (R2:{fit[r2s[0]]})')\n",
    "    print('_____________________________________________')\n",
    "    model = fit[r2s[1]]\n",
    "    type(model)\n",
    "\n",
    "    # Save the model with the highest fit metric\n",
    "    pickle.dump(model, open('regression_model.sav', 'wb'))\n",
    "\n",
    "def classification_selection(df, cols_to_drop=[]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import model_selection\n",
    "    from sklearn import preprocessing\n",
    "    import sklearn.neural_network as nn\n",
    "    from sklearn.linear_model import RidgeCV, LassoCV\n",
    "    import sklearn.ensemble as se\n",
    "    import sklearn.tree as tree\n",
    "    from sklearn import svm\n",
    "    import pickle\n",
    "\n",
    "    df = df.select_dtypes(np.number)  # Remove categorical features first\n",
    "    y = df.like_count                    # Save the label first\n",
    "    # Remove the label from the feature list\n",
    "    X = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Scale/normalize the features\n",
    "    X = pd.DataFrame(preprocessing.MinMaxScaler(\n",
    "    ).fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "    # Eyeball the data to make sure it looks right:\n",
    "    X_train\n",
    "\n",
    "    fit = {}  # Use this to store each of the fit metrics\n",
    "    # DECISION TREE MODELS: no assumptions about the data\n",
    "\n",
    "    # 1.16. Decision Tree Regression\n",
    "    model_dt = tree.DecisionTreeRegressor(random_state=12345)\n",
    "    model_dt.fit(X_train, y_train)\n",
    "    fit['Dec Tree R'] = model_dt.score(X_test, y_test)\n",
    "\n",
    "    # DECISION TREE-BASED ENSEMBLE MODELS: great for minimizing overfitting, these are based on averaging many unique sub-samples and combining algorithms\n",
    "    # 1.17. Decision Forrest\n",
    "    model_df = se.RandomForestRegressor(random_state=12345)\n",
    "    model_df.fit(X_train, y_train)\n",
    "    fit['Dec Forest R'] = model_df.score(X_test, y_test)\n",
    "\n",
    "    # 1.18. ExtraTreesRegressor\n",
    "    model_etr = se.ExtraTreesRegressor(random_state=12345)\n",
    "    model_etr.fit(X_train, y_train)\n",
    "    fit['Extra Trees R'] = model_etr.score(X_test, y_test)\n",
    "\n",
    "    # 1.19. AdaBoostRegressor\n",
    "    model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=12345)\n",
    "    model_abr.fit(X_train, y_train)\n",
    "    fit['AdaBoost DT R'] = model_abr.score(X_test, y_test)\n",
    "\n",
    "    # 1.20. GradientBoostingRegressor\n",
    "    model_gbr = se.GradientBoostingRegressor(random_state=12345)\n",
    "    model_gbr.fit(X_train, y_train)\n",
    "    fit['Grad. Boost R'] = model_gbr.score(X_test, y_test)\n",
    "\n",
    "    # 1.22. VotingRegressor: will combine other algorithms into an average; kind of cool\n",
    "    model_vr = se.VotingRegressor(estimators=[('DT', model_dt), ('DF', model_df), (\n",
    "        'ETR', model_etr), ('ABR', model_abr), ('GBR', model_gbr)])\n",
    "    model_vr.fit(X_train, y_train)\n",
    "    fit['Voting R'] = model_vr.score(X_test, y_test)\n",
    "\n",
    "    # 1.23. StackingRegressor\n",
    "    estimators = [('ridge', RidgeCV()), ('lasso', LassoCV(\n",
    "        random_state=42)), ('svr', svm.SVR(C=1, gamma=1e-6))]\n",
    "    model_sr = se.StackingRegressor(\n",
    "        estimators=estimators, final_estimator=se.GradientBoostingRegressor(random_state=12345))\n",
    "    model_sr.fit(X_train, y_train)\n",
    "    fit['Stacking R'] = model_sr.score(X_test, y_test)\n",
    "\n",
    "    # NEURAL-NETWORK MODELS: Based on deep learning methods\n",
    "\n",
    "    # 1.24. MLPRegressor\n",
    "    # Turn max_iter way up or down to get a more accurate result\n",
    "    model_nn = nn.MLPRegressor(max_iter=1000, random_state=12345)\n",
    "    model_nn.fit(X_train, y_train)\n",
    "    fit['NeuralNet R'] = model_nn.score(X_test, y_test)\n",
    "\n",
    "    # Sort and print the dictionary by greatest R squared to least\n",
    "    r2s = sorted_list_by_value = sorted(fit, key=fit.__getitem__, reverse=True)\n",
    "    for r2 in r2s:\n",
    "        print(f'{r2}:\\t{fit[r2]}')\n",
    "\n",
    "    # Select the model with the highest R squared\n",
    "    print(f'Best Classification Model: {r2s[0]} (R2:{fit[r2s[0]]})')\n",
    "    print('_____________________________________________')\n",
    "    model = fit[r2s[1]]\n",
    "    type(model)\n",
    "\n",
    "    # Save the model with the highest fit metric\n",
    "    pickle.dump(model, open('classification_model.sav', 'wb'))\n",
    "\n",
    "\n",
    "def clustering(df, cols_to_drop=[]):\n",
    "    import gower\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    import pickle\n",
    "\n",
    "    distance_matrix = gower.gower_matrix(df)\n",
    "    agg = AgglomerativeClustering(\n",
    "        affinity='precomputed', linkage='average').fit(distance_matrix)\n",
    "\n",
    "    # make a cluster column\n",
    "    df_wcluster = df.copy()\n",
    "    df_wcluster['cluster'] = agg.labels_\n",
    "    df_wcluster.head()\n",
    "\n",
    "    # Save the model with the highest fit metric\n",
    "    pickle.dump(df_wcluster, open('clustering_model.sav', 'wb'))\n",
    "\n",
    "    return df_wcluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\linear_model\\_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLars())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM R:\t-0.019500304021874193\n",
      "SGradientD R:\t-0.02422832069601455\n",
      "Bayesian R:\t-0.029338096245230094\n",
      "Lasso R:\t-0.03952092048524514\n",
      "LARS Lasso R:\t-0.03952092048524514\n",
      "NuSupportVM R:\t-0.114392221385607\n",
      "Ridge R:\t-0.1343226357054259\n",
      "OrdinaryLS R:\t-0.14279550656004414\n",
      "GaussianP R:\t-0.14279788246369352\n",
      "SupportVM R:\t-2.87696329258756\n",
      "Best Regression Model: Linear SVM R (R2:-0.019500304021874193)\n",
      "_____________________________________________\n",
      "Stacking R:\t-0.14810243691317915\n",
      "NeuralNet R:\t-0.46082323863004815\n",
      "Dec Forest R:\t-0.7172329638621173\n",
      "Grad. Boost R:\t-0.7511838963247328\n",
      "Voting R:\t-0.8031074996280321\n",
      "Extra Trees R:\t-0.9003485067212853\n",
      "Dec Tree R:\t-0.9904757969843101\n",
      "AdaBoost DT R:\t-1.4141394675381376\n",
      "Best Classification Model: Stacking R (R2:-0.14810243691317915)\n",
      "_____________________________________________\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>sentiment_overall</th>\n",
       "      <th>language_en</th>\n",
       "      <th>language_es</th>\n",
       "      <th>language_in</th>\n",
       "      <th>language_pt</th>\n",
       "      <th>source_Twitter Web App</th>\n",
       "      <th>source_Twitter for Android</th>\n",
       "      <th>source_Twitter for iPhone</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283954</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>0.175402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.688413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>530 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     like_count  possibly_sensitive  sentiment_overall  language_en  \\\n",
       "0      0.000000                 0.0           0.283954          1.0   \n",
       "1      0.000000                 0.0           0.664719          0.0   \n",
       "2      0.000000                 0.0           0.664719          1.0   \n",
       "3      0.000000                 0.0           1.000000          1.0   \n",
       "4      0.000000                 0.0           0.555774          1.0   \n",
       "..          ...                 ...                ...          ...   \n",
       "525    0.000000                 0.0           0.664719          0.0   \n",
       "526    0.000000                 0.0           0.699784          1.0   \n",
       "527    0.175402                 0.0           0.688413          0.0   \n",
       "528    0.000000                 0.0           0.555774          1.0   \n",
       "529    0.000000                 0.0           0.664719          0.0   \n",
       "\n",
       "     language_es  language_in  language_pt  source_Twitter Web App  \\\n",
       "0            0.0          0.0          0.0                     0.0   \n",
       "1            1.0          0.0          0.0                     0.0   \n",
       "2            0.0          0.0          0.0                     0.0   \n",
       "3            0.0          0.0          0.0                     0.0   \n",
       "4            0.0          0.0          0.0                     1.0   \n",
       "..           ...          ...          ...                     ...   \n",
       "525          0.0          0.0          1.0                     0.0   \n",
       "526          0.0          0.0          0.0                     0.0   \n",
       "527          1.0          0.0          0.0                     0.0   \n",
       "528          0.0          0.0          0.0                     1.0   \n",
       "529          0.0          1.0          0.0                     0.0   \n",
       "\n",
       "     source_Twitter for Android  source_Twitter for iPhone  cluster  \n",
       "0                           1.0                        0.0        0  \n",
       "1                           1.0                        0.0        0  \n",
       "2                           0.0                        1.0        0  \n",
       "3                           0.0                        1.0        0  \n",
       "4                           0.0                        0.0        0  \n",
       "..                          ...                        ...      ...  \n",
       "525                         1.0                        0.0        0  \n",
       "526                         0.0                        0.0        0  \n",
       "527                         1.0                        0.0        0  \n",
       "528                         0.0                        0.0        0  \n",
       "529                         0.0                        1.0        0  \n",
       "\n",
       "[530 rows x 11 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_modeling = df.copy()\n",
    "\n",
    "regression_selection(df_modeling, cols_to_drop=['like_count'])\n",
    "classification_selection(df_modeling, cols_to_drop=['like_count'])\n",
    "df_modeling = clustering(df_modeling)\n",
    "\n",
    "df_modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation\n",
    "* Dynamically select the best algorithm for your regression and classification models\n",
    "* Demonstrate through the feature importance metric which features should be included in the model. However, you don't need to set up an automated selection of features. You can manually decide which features to include\n",
    "* Dynamically save the best fitting model to a .sav file in the same folder as your .ipynb\n",
    "* Arrange your .ipynb file so that the \"Run all\" command will handle all steps above in order from data collection to .sav file"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ec8e2802e9ec64c1de1126b52a3a3eba2bbbc7f0465a520b33d3486dfa46c4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
