{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_tweets(bearer_token):\n",
    "    headers = {'Authorization': ('Bearer ' + bearer_token)}\n",
    "\n",
    "    n = 10000                             # The total number of tweets we want\n",
    "    max_results = 10 # The number of tweets to pull per request; must be between 10 and 100\n",
    "    total_retrieved = 0                   # To keep track of when to stop\n",
    "    next_token = \"\"                       # Must be empty on first iteration\n",
    "    search_term = \"manchester%20united\"\n",
    "\n",
    "    # Create empty DataFrames and set columns\n",
    "    df_tweets = pd.DataFrame(columns=['tweet_id', 'author_id', 'retweet_count', 'like_count',\n",
    "                                      'text', 'language', 'created_at', 'source', 'possibly_sensitive', 'image_url'])\n",
    "    df_users = pd.DataFrame(columns=['user_id', 'username', 'created_at', 'description', 'profile_image_url',\n",
    "                            'protected', 'verified', 'followers_count', 'following_count', 'tweet_count', 'listed_count'])\n",
    "\n",
    "    # stop when we have n results\n",
    "    while total_retrieved < n:\n",
    "\n",
    "        # the first time through the loop, we do not need the next_token parameter\n",
    "        if next_token == \"\":\n",
    "            url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}'\n",
    "        else:\n",
    "            url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&next_token={next_token}'\n",
    "\n",
    "        # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\n",
    "        url += f'&expansions=geo.place_id,author_id,attachments.media_keys'\n",
    "        url += f'&tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,referenced_tweets,reply_settings,source,text,withheld'\n",
    "        url += f'&media.fields=media_key,type,url&user.fields=created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld'\n",
    "\n",
    "        # make the request to the Twitter API Recent Search endpoint\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "        try:  # Just in case we get an error\n",
    "            json_data = json.loads(response.text)\n",
    "            # print(json_data)\n",
    "        except:\n",
    "            print(response.text)\n",
    "\n",
    "        for tweet in json_data['data']:\n",
    "            media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\n",
    "\n",
    "            # Store the data into variables\n",
    "            tweet_id = tweet['id']\n",
    "            author_id = tweet['author_id']\n",
    "            retweet_count = tweet['public_metrics']['retweet_count']    # label\n",
    "            like_count = tweet['public_metrics']['like_count']          # label\n",
    "            image_url = \"\"                                              # image\n",
    "            text = tweet['text']                                        # text\n",
    "            created_at = tweet['created_at']                            # categorical\n",
    "            source = tweet['source']                                    # categorical\n",
    "            possibly_sensitive = tweet['possibly_sensitive']            # categorical\n",
    "            language = tweet['lang']                                    # categorical\n",
    "\n",
    "            # Find out if there is media\n",
    "            if 'attachments' in tweet:\n",
    "                if 'media_keys' in tweet['attachments']:\n",
    "                    media_key = tweet['attachments']['media_keys'][0]\n",
    "\n",
    "            # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\n",
    "            if media_key != \"\":\n",
    "                for media in json_data['includes']['media']:\n",
    "                    # Only if the media_key matches the one we stored\n",
    "                    if media['media_key'] == media_key:\n",
    "                        if media['type'] == 'photo':      # Only if it is a photo; ignore videos\n",
    "                            # Store the url in a variable\n",
    "                            image_url = media['url']\n",
    "\n",
    "            # Add the new data to a new record in the DataFrame\n",
    "            df_tweets.loc[tweet_id] = [tweet_id, author_id, retweet_count, like_count,\n",
    "                                       text, language, created_at, source, possibly_sensitive, image_url]\n",
    "\n",
    "        # keep track of how many results have been obtained so far:\n",
    "        total_retrieved += 10\n",
    "\n",
    "        # keep track of where to start next time, but quit if there are no more results\n",
    "        try:\n",
    "            next_token = json_data['meta']['next_token']\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        # get user info\n",
    "        for user in json_data['includes']['users']:\n",
    "            user_id = user['id']\n",
    "            user_name = user['username']\n",
    "            user_created_at = user['created_at']\n",
    "            user_description = user['description']\n",
    "            user_profile_image_url = user['profile_image_url']\n",
    "            user_protected = user['protected']\n",
    "            user_verified = user['verified']\n",
    "            user_followers_count = user['public_metrics']['followers_count']\n",
    "            user_following_count = user['public_metrics']['following_count']\n",
    "            user_tweet_count = user['public_metrics']['tweet_count']\n",
    "            user_listed_count = user['public_metrics']['listed_count']\n",
    "\n",
    "            # put user info into a user dataframe\n",
    "            df_users.loc[user_id] = [user_id, user_name, user_created_at, user_description, user_profile_image_url,\n",
    "                                     user_protected, user_verified, user_followers_count, user_following_count, user_tweet_count, user_listed_count]\n",
    "\n",
    "        # sleep to avoid hitting the rate limit\n",
    "        time.sleep(10)\n",
    "\n",
    "    # All done! save the dataframes to csv files\n",
    "    df_tweets.to_csv('tweets.csv')\n",
    "    df_users.to_csv('users.csv')\n",
    "    print('Got all the tweets!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets('AAAAAAAAAAAAAAAAAAAAAGnaTwEAAAAAhRdM6yLmei6skyaWcjbx8IDFnlw%3DLPQHO2CTw1nVjjHLx3htgP9qmeCOgPpt96EdDujokNcWljI5iP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Cleaning and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ec8e2802e9ec64c1de1126b52a3a3eba2bbbc7f0465a520b33d3486dfa46c4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
