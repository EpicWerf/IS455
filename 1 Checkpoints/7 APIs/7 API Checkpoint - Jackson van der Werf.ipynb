{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Tweets about Manchester United"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got all the tweets!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAGnaTwEAAAAAhRdM6yLmei6skyaWcjbx8IDFnlw%3DLPQHO2CTw1nVjjHLx3htgP9qmeCOgPpt96EdDujokNcWljI5iP'\n",
    "headers = {'Authorization':('Bearer '+ bearer_token)}\n",
    "\n",
    "n = 10000                             # The total number of tweets we want\n",
    "max_results = 10                      # The number of tweets to pull per request; must be between 10 and 100\n",
    "total_retrieved = 0                   # To keep track of when to stop\n",
    "next_token = \"\"                       # Must be empty on first iteration\n",
    "search_term = \"manchester%20united\"   # To form an advanced query, see here: https://twitter.com/search-advanced?lang=en\n",
    "\n",
    "# Create empty DataFrames and set columns\n",
    "df_tweets = pd.DataFrame(columns=['tweet_id', 'author_id', 'retweet_count', 'like_count', 'text', 'language', 'created_at', 'source', 'possibly_sensitive', 'image_url'])\n",
    "df_users = pd.DataFrame(columns=['user_id', 'username', 'created_at', 'description', 'profile_image_url', 'protected', 'verified', 'followers_count', 'following_count', 'tweet_count', 'listed_count'])\n",
    "\n",
    "# stop when we have n results\n",
    "while total_retrieved < n:\n",
    "\n",
    "  # the first time through the loop, we do not need the next_token parameter\n",
    "  if next_token == \"\":\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}'\n",
    "  else:\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&next_token={next_token}'\n",
    "\n",
    "  # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\n",
    "  url += f'&expansions=geo.place_id,author_id,attachments.media_keys'\n",
    "  url += f'&tweet.fields=attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,referenced_tweets,reply_settings,source,text,withheld'\n",
    "  url += f'&media.fields=media_key,type,url&user.fields=created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld'\n",
    "\n",
    "\n",
    "  # make the request to the Twitter API Recent Search endpoint\n",
    "  response = requests.request(\"GET\", url, headers=headers)\n",
    "  try:  # Just in case we get an error\n",
    "    json_data = json.loads(response.text)\n",
    "    # print(json_data)\n",
    "  except:\n",
    "    print(response.text)\n",
    "\n",
    "  for tweet in json_data['data']:\n",
    "    media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\n",
    "\n",
    "    # Store the data into variables\n",
    "    tweet_id = tweet['id']\n",
    "    author_id = tweet['author_id']                               \n",
    "    retweet_count = tweet['public_metrics']['retweet_count']     #label\n",
    "    like_count = tweet['public_metrics']['like_count']           #label\n",
    "    image_url = \"\"                                               #image\n",
    "    text = tweet['text']                                         #text\n",
    "    created_at = tweet['created_at']                             #categorical\n",
    "    source = tweet['source']                                     #categorical\n",
    "    possibly_sensitive = tweet['possibly_sensitive']             #categorical\n",
    "    language = tweet['lang']                                     #categorical\n",
    "\n",
    "    # Find out if there is media\n",
    "    if 'attachments' in tweet:\n",
    "      if 'media_keys' in tweet['attachments']:\n",
    "        media_key = tweet['attachments']['media_keys'][0]\n",
    "\n",
    "    # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\n",
    "    if media_key != \"\":\n",
    "      for media in json_data['includes']['media']:\n",
    "        if media['media_key'] == media_key: # Only if the media_key matches the one we stored\n",
    "          if media['type'] == 'photo':      # Only if it is a photo; ignore videos\n",
    "            image_url = media['url']        # Store the url in a variable\n",
    "\n",
    "    # Add the new data to a new record in the DataFrame\n",
    "    df_tweets.loc[tweet_id] = [tweet_id, author_id, retweet_count, like_count, text, language, created_at, source, possibly_sensitive, image_url]\n",
    "\n",
    "  # keep track of how many results have been obtained so far:\n",
    "  total_retrieved += 10\n",
    "\n",
    "  # keep track of where to start next time, but quit if there are no more results\n",
    "  try:\n",
    "    next_token = json_data['meta']['next_token']\n",
    "  except:\n",
    "    break  \n",
    "\n",
    "  # get user info\n",
    "  for user in json_data['includes']['users']:\n",
    "    user_id = user['id']\n",
    "    user_name = user['username']\n",
    "    user_created_at = user['created_at']\n",
    "    user_description = user['description']\n",
    "    user_profile_image_url = user['profile_image_url']\n",
    "    user_protected = user['protected']\n",
    "    user_verified = user['verified']\n",
    "    user_followers_count = user['public_metrics']['followers_count']\n",
    "    user_following_count = user['public_metrics']['following_count']\n",
    "    user_tweet_count = user['public_metrics']['tweet_count']\n",
    "    user_listed_count = user['public_metrics']['listed_count']\n",
    "\n",
    "    #put user info into a user dataframe\n",
    "    df_users.loc[user_id] = [user_id, user_name, user_created_at, user_description, user_profile_image_url, user_protected, user_verified, user_followers_count, user_following_count, user_tweet_count, user_listed_count]\n",
    "  \n",
    "  #sleep to avoid hitting the rate limit\n",
    "  time.sleep(10)\n",
    "\n",
    "print('Got all the tweets!')\n",
    "\n",
    "#set df indexes. I\"m not doing that here as it messes up the indexes when sending it to a sqlite database\n",
    "# df_tweets.set_index('tweet_id', inplace=True)\n",
    "# df_users.set_index('user_id', inplace=True)\n",
    "\n",
    "# df_tweets.to_csv('tweets.csv')\n",
    "# df_users.to_csv('users.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully wrote to DB!\n"
     ]
    }
   ],
   "source": [
    "# Read data from my server and store into a new sqlite3 database\n",
    "import pyodbc\n",
    "import sqlite3\n",
    "\n",
    "conn_write = sqlite3.connect('twitter.db')\n",
    "df_tweets.to_sql(name='tweets', con=conn_write, if_exists='replace', index=False)\n",
    "df_users.to_sql(name='users', con=conn_write, if_exists='replace', index=False)\n",
    "\n",
    "conn_write.close()\n",
    "\n",
    "print('Succesfully wrote to DB!')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8ec8e2802e9ec64c1de1126b52a3a3eba2bbbc7f0465a520b33d3486dfa46c4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
