{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqv1onoK1Ogw"
      },
      "source": [
        "## **Prior Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KXJwScpq1MvG"
      },
      "outputs": [],
      "source": [
        "def get_data(url, drop=[]):\n",
        "  import pandas as pd\n",
        "  df = pd.read_csv(url)\n",
        "  if len(drop) > 0:\n",
        "    for col in drop:\n",
        "      df.drop(columns=[col], inplace=True)\n",
        "  return df\n",
        "\n",
        "def bin_groups(df, percent=.05):\n",
        "  import pandas as pd\n",
        "  for col in df:\n",
        "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "      for group, count in df[col].value_counts().iteritems():\n",
        "        if count / len(df) < percent:\n",
        "          df.loc[df[col] == group, col] = 'Other'\n",
        "  return df\n",
        "\n",
        "def drop_columns_missing_data(df, cutoff=.5):\n",
        "  import pandas as pd\n",
        "  for col in df:\n",
        "    if df[col].isna().sum() / len(df) > cutoff:\n",
        "      df.drop(columns=[col], inplace=True)\n",
        "  return df\n",
        "\n",
        "def impute_mean(df):\n",
        "  from sklearn.impute import SimpleImputer\n",
        "  import pandas as pd, numpy as np\n",
        "  for col in df:\n",
        "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "  imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
        "  return df\n",
        "\n",
        "def impute_KNN(df):\n",
        "  from sklearn.impute import KNNImputer\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  import pandas as pd\n",
        "  for col in df:\n",
        "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "  df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns = df.columns)\n",
        "  imp = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
        "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
        "  return df\n",
        "\n",
        "def impute_reg(df):\n",
        "  from sklearn.experimental import enable_iterative_imputer\n",
        "  from sklearn.impute import IterativeImputer\n",
        "  import pandas as pd\n",
        "  for col in df:\n",
        "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "      df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "  imp = IterativeImputer(max_iter=10, random_state=12345)\n",
        "  df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
        "  return df\n",
        "\n",
        "def fit_mlr(df, test_size=.2, random_state=12345, label=''):\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  import pandas as pd\n",
        "  X = df.drop(label,axis=1)\n",
        "  y = df[label]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "  model = LinearRegression().fit(X_train, y_train)\n",
        "  print(f'R-squared (mlr): \\t{model.score(X_test, y_test)}')\n",
        "  return model\n",
        "\n",
        "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
        "  import pandas as pd\n",
        "  from numpy import mean, std\n",
        "  X = df.drop(label,axis=1)\n",
        "  y = df[label]\n",
        "  if repeat:\n",
        "    cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=12345)\n",
        "  else:\n",
        "    cv = KFold(n_splits=10, random_state=12345, shuffle=True)\n",
        "  scores = cross_val_score(LinearRegression(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
        "  print(f'Average R-squared:\\t{mean(scores)}')\n",
        "  return LinearRegression().fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2xcAugbLrEJ"
      },
      "source": [
        "# **Feature Importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk9xvaa4LrKS",
        "outputId": "b42b9bff-97e1-49d7-d87f-74e1fc2ecf5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average R-squared:\t0.797855361810675\n"
          ]
        }
      ],
      "source": [
        "#Data cleaning and preperation pipeline\n",
        "df = get_data('http://www.ishelp.info/data/housing_full.csv', ['Id'])\n",
        "df = bin_groups(df)\n",
        "df = drop_columns_missing_data(df, cutoff=.45)\n",
        "df = impute_mean(df)\n",
        "\n",
        "#Modeling pipeline\n",
        "    #use 5 instead of 10 for the Final Project\n",
        "model = fit_crossvalidate_mlr(df, 10, label='SalePrice')\n",
        "\n",
        "#Deployment pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz01YNDDxwit"
      },
      "source": [
        "### **Simple bivariate feature importance**\n",
        "\n",
        "Pearson correlation, t-test, or chi square"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gu-77cVouyOO",
        "outputId": "9b1c948a-1116-4a38-dc04-8ea6eccfd5ed"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(df.corr())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv4eNsD1x8pF"
      },
      "source": [
        "### **Multivariate feature importance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT6WWxA1yGbe"
      },
      "source": [
        "Linear Model: Coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-U2nasmtR-wk",
        "outputId": "034d9359-2184-4f51-f87f-44c0325e2827"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "coefs = pd.DataFrame(\n",
        "   model.coef_,\n",
        "   columns=['Coefficients'], index=df.drop(columns=['SalePrice']).columns\n",
        ")\n",
        "\n",
        "coefs.sort_values(by=['Coefficients'], ascending=False, inplace=True)\n",
        "\n",
        "coefs.plot(kind='barh', figsize=(10, 30))\n",
        "plt.title('MLR model')\n",
        "plt.axvline(x=0, color='.5')\n",
        "plt.subplots_adjust(left=.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "chpj8M0UPuNt",
        "outputId": "ac4ad9e2-47e9-4b07-80a9-aaf3ce87c621"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Standardize the data to compare feature importances\n",
        "df_minmax = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(df), columns=df.columns)\n",
        "\n",
        "# Run our fit_crossvalidate_mlr function to get back a trained model\n",
        "model = fit_crossvalidate_mlr(df_minmax, 10, label='SalePrice')\n",
        "\n",
        "coefs = pd.DataFrame(\n",
        "   model.coef_,\n",
        "   columns=['Coefficients'], index=df_minmax.drop(columns=['SalePrice']).columns\n",
        ")\n",
        "\n",
        "coefs.sort_values(by=['Coefficients'], ascending=False, inplace=True)\n",
        "\n",
        "coefs.plot(kind='barh', figsize=(10, 30))\n",
        "plt.title('MLR model')\n",
        "plt.axvline(x=0, color='.5')\n",
        "plt.subplots_adjust(left=.3)\n",
        "\n",
        "# Baseline R2: 0.7975108601968853"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "AyT8HQ1JkVVZ",
        "outputId": "9618f645-e213-468a-a413-fb6c5b17fecc"
      },
      "outputs": [],
      "source": [
        "def vif(df, label=\"\"):\n",
        "  import pandas as pd\n",
        "  from sklearn.linear_model import LinearRegression\n",
        "  \n",
        "  # initialize dictionaries\n",
        "  vif_dict, tolerance_dict = {}, {}\n",
        "\n",
        "  # drop unnecessary columns if they are found in the dataframe\n",
        "  if label in df.columns: df.drop(columns=[label], inplace=True)\n",
        "  if 'const' in df.columns: df.drop(columns=['const'], inplace=True)\n",
        "\n",
        "  # form input data for each exogenous variable\n",
        "  for col in df:\n",
        "    y = df[col]\n",
        "    X = df.drop(columns=[col])\n",
        "    \n",
        "    # extract r-squared from the fit\n",
        "    r_squared = LinearRegression().fit(X, y).score(X, y)\n",
        "\n",
        "    # calculate VIF\n",
        "    if r_squared < 1: # Prevent division by zero runtime error\n",
        "      vif = 1/(1 - r_squared) \n",
        "    else:\n",
        "      vif = 100\n",
        "    vif_dict[col] = vif\n",
        "\n",
        "    # calculate tolerance\n",
        "    tolerance = 1 - r_squared\n",
        "    tolerance_dict[col] = tolerance\n",
        "\n",
        "  # generate the DataFrame to return\n",
        "  return pd.DataFrame({'VIF': vif_dict, 'Tolerance': tolerance_dict}).sort_values(by=['VIF'], ascending=False)\n",
        "\n",
        "vif(df).head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wcHqUjPyLLF"
      },
      "source": [
        "Tree Model: Permutation Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN6MEQJmPuT7",
        "outputId": "dce83333-ffd3-46b6-8780-2af4c6a045b0"
      },
      "outputs": [],
      "source": [
        "# Multivariate feature importance for tree model\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
        "from numpy import mean\n",
        "\n",
        "y = df.SalePrice\n",
        "X = df.drop(columns=['SalePrice'])\n",
        "\n",
        "model = DecisionTreeRegressor()\n",
        "model.fit(X, y)\n",
        "\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=12345)\n",
        "scores = cross_val_score(DecisionTreeRegressor(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
        "print(f'Decision Tree R-squared:\\t{mean(scores)}')\n",
        "\n",
        "# Baseline R2:      0.7975108601968853"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lnnHcR3Xysv4",
        "outputId": "eef5f107-e77f-498c-84a1-693932988ed8"
      },
      "outputs": [],
      "source": [
        "df_fi = pd.DataFrame({\"Feature Importance\":model.feature_importances_}, index=df.drop(columns=['SalePrice']).columns)\n",
        "df_fi.sort_values(by=['Feature Importance'], ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "M56NcUX-8Lkc",
        "outputId": "dfe59815-f1c8-411c-b97f-83d997e5ade6"
      },
      "outputs": [],
      "source": [
        "import matplotlib, pandas as pd\n",
        "model.feature_importances_\n",
        "\n",
        "matplotlib.rc('figure', figsize=[5,5])\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "feat_importances = pd.Series(model.feature_importances_, index=X_test.columns)\n",
        "feat_importances.nlargest(20).plot(kind='barh',title = 'Feature Importance')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhM_aKvy9vTp"
      },
      "source": [
        "# **Feature Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o4UXfhOL06J"
      },
      "source": [
        "## **Univariate Techniques**\n",
        "These techniques are simple and quick; potentially useful when there are an extremely high number of features and the most impactful features changes over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQgjsiyxL6i"
      },
      "source": [
        "### **Variance Threshold**\n",
        "Remove features with variance explained below threshold n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjv-eJE6xMBx",
        "outputId": "8244b387-1ffd-436e-dc6c-a0b19dc9e066"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import pandas as pd\n",
        "\n",
        "# Import data and follow prior cleaning steps\n",
        "df = get_data('http://www.ishelp.info/data/housing_full.csv', ['Id'])\n",
        "df = bin_groups(df)\n",
        "df = drop_columns_missing_data(df)\n",
        "df = impute_mean(df)\n",
        "\n",
        "# Select features only\n",
        "X = df.drop(columns=['SalePrice'])\n",
        "\n",
        "# View the total number of features before reducing\n",
        "print(X.shape)\n",
        "\n",
        "# Calculate variance based on desired p-value so that Variance = p(1 - p)\n",
        "# The code below uses p = 0.8 as the cutoff value:\n",
        "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
        "sel.fit_transform(X)\n",
        "X_reduced = pd.DataFrame(sel.fit_transform(X))\n",
        "\n",
        "# View the remaining number of features\n",
        "print(X_reduced.shape)\n",
        "\n",
        "# Notice that columns are dropped\n",
        "X_reduced.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oFjiF8b_26k",
        "outputId": "00f1247e-8b0f-4102-dde3-51438d383a4a"
      },
      "outputs": [],
      "source": [
        "# To see which columns were retained:\n",
        "sel.get_feature_names_out()\n",
        "df[sel.get_feature_names_out()].columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ju5rzT5__7x",
        "outputId": "ffb31af2-2a1e-4d27-9132-b173a731010d"
      },
      "outputs": [],
      "source": [
        "def fs_variance(df, label=\"\", p=0.8):\n",
        "  from sklearn.feature_selection import VarianceThreshold\n",
        "  import pandas as pd\n",
        "\n",
        "  if label != \"\":\n",
        "    X = df.drop(columns=[label])\n",
        "    \n",
        "  sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
        "  sel.fit_transform(X)\n",
        "\n",
        "  # Add the label back in after removing poor features\n",
        "  return df[sel.get_feature_names_out()].join(df[label])\n",
        "\n",
        "\n",
        "# Integrate this new function into the pipeline:\n",
        "df = get_data('http://www.ishelp.info/data/housing_full.csv', ['Id'])\n",
        "df = bin_groups(df)\n",
        "df = drop_columns_missing_data(df)\n",
        "df = impute_mean(df)\n",
        "df = fs_variance(df, label=\"SalePrice\")\n",
        "model = fit_crossvalidate_mlr(df, 10, \"SalePrice\")\n",
        "\n",
        "# MLR all features R2: 0.7975108601968853"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgAhNe0X5qPm"
      },
      "source": [
        "### **SelectKBest**\n",
        "Select the top k features based on chosen bivariate metric: R2 or Chi2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHMhzTIfP4hM",
        "outputId": "1c9c42b6-40d0-4c73-a2b5-cb31c716cfd3"
      },
      "outputs": [],
      "source": [
        "def fs_kbest(df, k=10, label=\"\"):\n",
        "  from sklearn.feature_selection import SelectKBest\n",
        "  from sklearn.feature_selection import r_regression\n",
        "  import pandas as pd\n",
        "\n",
        "  X = df.drop(columns=[label])\n",
        "  y = df[label]\n",
        "\n",
        "  # Select the top k features based on a given bivariate metric\n",
        "  sel = SelectKBest(r_regression, k=k)\n",
        "  sel.fit_transform(X, y)\n",
        "  \n",
        "  return df[sel.get_feature_names_out()].join(df[label])\n",
        "\n",
        "# Integrate this new function into the pipeline:\n",
        "df = get_data('http://www.ishelp.info/data/housing_full.csv', ['Id'])\n",
        "df = bin_groups(df)\n",
        "df = drop_columns_missing_data(df)\n",
        "df = impute_mean(df)\n",
        "df = fs_kbest(df, 58, label=\"SalePrice\")\n",
        "model = fit_crossvalidate_mlr(df, 10, \"SalePrice\")\n",
        "\n",
        "# MLR all features R2:  0.7975108601968853\n",
        "# - low variance feats: 0.7891806836088955"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsnv7ihW52IO"
      },
      "source": [
        "## **Recursive Feature Selection**\n",
        "Remove the worst feature, then rerun effect size metrics, repeat. This technique is slower, but much more accurate. Use these techniques if you have either adequate processing time or power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBxklKA76vtT"
      },
      "source": [
        "### **L1-based feature selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p_7TKvT0NEK",
        "outputId": "84b8ccaf-17a0-42da-a655-13899ddff4af"
      },
      "outputs": [],
      "source": [
        "def fs_select_linear(df, label=\"\"):\n",
        "  from sklearn.svm import LinearSVC\n",
        "  from sklearn.feature_selection import SelectFromModel\n",
        "  import pandas as pd\n",
        "\n",
        "  X = df.drop(label,axis=1)\n",
        "  y = df[label]\n",
        "\n",
        "  # As C increases, more features are kept\n",
        "  lsvc = LinearSVC(C=0.05, penalty=\"l1\", dual=False).fit(X, y)\n",
        "  sel = SelectFromModel(lsvc, prefit=True)\n",
        "  sel.transform(X)\n",
        "\n",
        "  columns = list(X.columns[sel.get_support()])\n",
        "  columns.append(label)\n",
        "  return df[columns]\n",
        "\n",
        "# Integrate this new function into the pipeline:\n",
        "df = get_data('http://www.ishelp.info/data/housing_full.csv', ['Id'])\n",
        "df = bin_groups(df)\n",
        "df = drop_columns_missing_data(df)\n",
        "df = impute_mean(df)\n",
        "df = fs_select_linear(df, label=\"SalePrice\")\n",
        "model = fit_crossvalidate_mlr(df, 10, \"SalePrice\")\n",
        "\n",
        "# MLR all features R2:  0.7975108601968853\n",
        "# - low variance feats: 0.7891806836088955\n",
        "# Keep 58 best feats:   0.7816135597350667"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIw9EPzv7SNA"
      },
      "source": [
        "### **Tree-based feature selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpj1rMY17JR4",
        "outputId": "428af4f8-448b-452c-8094-6c877b9992d3"
      },
      "outputs": [],
      "source": [
        "def fs_select_trees(df, label=\"SalePrice\"):\n",
        "  from sklearn.ensemble import ExtraTreesClassifier\n",
        "  from sklearn.feature_selection import SelectFromModel\n",
        "  import pandas as pd\n",
        "\n",
        "  X = df.drop(columns=[label])\n",
        "  y = df[label]\n",
        "\n",
        "  clf = ExtraTreesClassifier(n_estimators=50)\n",
        "  clf = clf.fit(X, y)\n",
        "  sel = SelectFromModel(clf, prefit=True)\n",
        "  sel.transform(X)\n",
        "\n",
        "  columns = list(X.columns[sel.get_support()])\n",
        "  columns.append(label)\n",
        "  return df[columns]\n",
        "\n",
        "# Integrate this new function into the pipeline:\n",
        "df = get_data('http://www.ishelp.info/data/housing_full.csv', ['Id'])\n",
        "df = bin_groups(df)\n",
        "df = drop_columns_missing_data(df)\n",
        "df = impute_mean(df)\n",
        "df = fs_select_trees(df, label=\"SalePrice\")\n",
        "model = fit_crossvalidate_mlr(df, 10, \"SalePrice\")\n",
        "\n",
        "# MLR all features R2:      0.7975108601968853\n",
        "# - low variance feats:     0.7891806836088955\n",
        "# Keep 58 best feats:       0.7816135597350667\n",
        "# L1 recursive selection:   0.7637683060167728"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9ZY8rQL8NiI"
      },
      "outputs": [],
      "source": [
        "# MLR all features R2:      0.7975108601968853\n",
        "# - low variance feats:     0.7891806836088955\n",
        "# Keep 58 best feats:       0.7816135597350667\n",
        "# L1 recursive selection:   0.7637683060167728\n",
        "# Tree-based recursive:     0.7824110291359192"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ML 2: Feature Selection.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
