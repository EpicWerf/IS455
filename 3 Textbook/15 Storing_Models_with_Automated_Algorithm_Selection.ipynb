{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Storing Models with Automated Algorithm Selection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-dS89lRZK7q"
      },
      "source": [
        "# **Storing Models with Automated Algorithm Selection with Model Persistence**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY-E92fEZXLP"
      },
      "source": [
        "## Import, Dummy Code, Normalize, and Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow7pbMQalemk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "a1e75846-ed3f-4155-b6ea-25e38a56bc17"
      },
      "source": [
        "# STEP 1: Get the data, dummy code it, standardize it, divide and split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "\n",
        "df = pd.read_csv('https://www.ishelp.info/data/insurance.csv')\n",
        "# Better -> pull from live DB: SELECT TOP(100000) FROM insurance ORDER BY date DESC\n",
        "\n",
        "# Generate dummy codes\n",
        "for col in df:\n",
        "  if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "    df = df.join(pd.get_dummies(df[col], prefix=col, drop_first=True))\n",
        "\n",
        "df = df.select_dtypes(np.number)  # Remove categorical features first\n",
        "y = df.charges                    # Save the label first\n",
        "X = df.drop(columns=['charges'])   # Remove the label from the feature list\n",
        "\n",
        "# Scale/normalize the features\n",
        "X = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=12345)\n",
        "\n",
        "# Eyeball the data to make sure it looks right:\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>smoker_yes</th>\n",
              "      <th>region_northwest</th>\n",
              "      <th>region_southeast</th>\n",
              "      <th>region_southwest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1046</th>\n",
              "      <td>0.543478</td>\n",
              "      <td>0.245359</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>0.108696</td>\n",
              "      <td>0.698144</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1144</th>\n",
              "      <td>0.695652</td>\n",
              "      <td>0.439602</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522</th>\n",
              "      <td>0.717391</td>\n",
              "      <td>0.483051</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.478261</td>\n",
              "      <td>0.342480</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>0.217391</td>\n",
              "      <td>0.523944</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>0.804348</td>\n",
              "      <td>0.458434</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>0.434783</td>\n",
              "      <td>0.504170</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1309</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.436911</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.414044</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>936 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           age       bmi  ...  region_southeast  region_southwest\n",
              "1046  0.543478  0.245359  ...               0.0               0.0\n",
              "358   0.108696  0.698144  ...               1.0               0.0\n",
              "1144  0.695652  0.439602  ...               0.0               1.0\n",
              "522   0.717391  0.483051  ...               0.0               0.0\n",
              "54    0.478261  0.342480  ...               0.0               0.0\n",
              "...        ...       ...  ...               ...               ...\n",
              "546   0.217391  0.523944  ...               0.0               0.0\n",
              "382   0.804348  0.458434  ...               1.0               0.0\n",
              "129   0.434783  0.504170  ...               0.0               1.0\n",
              "1309  0.500000  0.436911  ...               0.0               1.0\n",
              "482   0.000000  0.414044  ...               1.0               0.0\n",
              "\n",
              "[936 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWUSgMM1Zi0p"
      },
      "source": [
        "## Algorithm/Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvUYx-cECXN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a2514d-f338-4c55-e66e-82f9ee2ca403"
      },
      "source": [
        "# ALGORITHMS: See a complete list of regression algorithems in Sci-Kit Learn: https://scikit-learn.org/stable/supervised_learning.html\n",
        "\n",
        "fit = {}  # Use this to store each of the fit metrics\n",
        "\n",
        "# 1. LINEAR MODELS: assumes normal distribution, homoscedasticity, no multi-collinearity, independence, and no auto-correlation (some exceptions apply)\n",
        "import sklearn.linear_model as lm\n",
        "\n",
        "# 1.1. Ordinary Least Squares Multiple Linear Regression\n",
        "model_ols = lm.LinearRegression()\n",
        "model_ols.fit(X_train, y_train)\n",
        "fit['OrdinaryLS R'] = model_ols.score(X_test, y_test)\n",
        "\n",
        "# 1.2. Ridge Regression: more robust to multi-collinearity\n",
        "model_rr = lm.Ridge(alpha=0.5) # adjust this alpha parameter for better results (between 0 and 1)\n",
        "model_rr.fit(X_train, y_train)\n",
        "fit['Ridge R'] = model_rr.score(X_test, y_test)\n",
        "\n",
        "# 1.3. Lasso Regression: better for sparse values like RetweetCount where most are zeros but a few have many retweets.\n",
        "model_lr = lm.Lasso(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
        "model_lr.fit(X_train, y_train)\n",
        "fit['Lasso R'] = model_lr.score(X_test, y_test)\n",
        "\n",
        "# 1.4. Least Angle Regression: good when the number of features is greater than the number of samples\n",
        "model_llr = lm.LassoLars(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
        "model_llr.fit(X_train, y_train)\n",
        "fit['LARS Lasso R'] = model_llr.score(X_test, y_test)\n",
        "\n",
        "# 1.5. Bayesian Regression: probability based; allows regularization parameters, automatically tuned to data\n",
        "model_br = lm.BayesianRidge()\n",
        "model_br.fit(X_train, y_train)\n",
        "fit['Bayesian R'] = model_br.score(X_test, y_test)\n",
        "\n",
        "# These only work on sklearn 0.23 which is not available in Colab yet. You can use these on your Jupyter or VS Code versions\n",
        "# # 1.6. Generalized Linear Regression (Poisson): Good when you don't have a normal distribution, count-based data, and a Poisson distribution\n",
        "# model_pr = sklearn.linear_model.TweedieRegressor(power=1, link=\"log\") # Power=1 means this is a Poisson\n",
        "# model_pr.fit(X_train, y_train)\n",
        "# fit['Poisson R'] = model_pr.score(X_test, y_test)\n",
        "\n",
        "# # 1.7. Generalized Linear Regression (Gamma): Good when you don't have a normal distribution, continuous data, and a Gamma distribution\n",
        "# model_gr = lm.TweedieRegressor(power=2, link=\"log\") # Power=2 means this is a Gamma\n",
        "# model_gr.fit(X_train, y_train)\n",
        "# fit['Gamma R'] = model_gr.score(X_test, y_test)\n",
        "\n",
        "# # 1.8. Generalized Linear Regression (Inverse Gamma): Good when you don't have a normal distribution, continuous data, and an inverse Gamma distribution\n",
        "# model_igr = lm.TweedieRegressor(power=3) # Power=3 means this is an inverse Gamma\n",
        "# model_igr.fit(X_train, y_train)\n",
        "# fit['Inverse Gamma R'] = model_igr.score(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "# SUPPORT VECTOR MACHINES\n",
        "from sklearn import svm\n",
        "\n",
        "# 1.9. SVM: this is the default SVM, parameters can be modified to make this more accurate\n",
        "model_svm = svm.SVR()\n",
        "model_svm.fit(X_train, y_train)\n",
        "fit['SupportVM R'] = model_svm.score(X_test, y_test)\n",
        "\n",
        "# 1.10. Linear SVM: Faster than SVM but only considers a linear model\n",
        "model_lsvm = svm.LinearSVR()\n",
        "model_lsvm.fit(X_train, y_train)\n",
        "fit['Linear SVM R'] = model_lsvm.score(X_test, y_test)\n",
        "\n",
        "# 1.11. NuSVM: \n",
        "model_nusvm = svm.NuSVR()\n",
        "model_nusvm.fit(X_train, y_train)\n",
        "fit['NuSupportVM R'] = model_nusvm.score(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# STOCHASTIC GRADIENT DESCENT REGRESSION\n",
        "# 1.12. SGDRegressor: \n",
        "model_sgdr = lm.SGDRegressor()\n",
        "model_sgdr.fit(X_train, y_train)\n",
        "fit['SGradientD R'] = model_sgdr.score(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# KNN: NEAREST NEIGHBORS REGRESSION\n",
        "from sklearn import neighbors\n",
        "\n",
        "# 1.13. KNeighborsRegressor: \n",
        "model_knnr = neighbors.KNeighborsRegressor(5, 'uniform')\n",
        "model_knnr.fit(X_train, y_train)\n",
        "fit['KNNeighbors R'] = model_knnr.score(X_test, y_test)\n",
        "\n",
        "# 1.14. KNeighborsRegressor: \n",
        "model_knnrd = neighbors.KNeighborsRegressor(8, 'distance')\n",
        "model_knnrd.fit(X_train, y_train)\n",
        "fit['KNNeighborsD R'] = model_knnrd.score(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# GAUSSIAN PROCESS REGRESSION\n",
        "from sklearn import gaussian_process\n",
        "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
        "\n",
        "# 1.15. GaussianProcessRegressor:\n",
        "model_gpr = gaussian_process.GaussianProcessRegressor(DotProduct() + WhiteKernel())\n",
        "model_gpr.fit(X_train, y_train)\n",
        "fit['GaussianP R'] = model_gpr.score(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# DECISION TREE MODELS: no assumptions about the data\n",
        "import sklearn.tree as tree\n",
        "import sklearn.ensemble as se\n",
        "\n",
        "# 1.16. Decision Tree Regression\n",
        "model_dt = tree.DecisionTreeRegressor(random_state=12345)\n",
        "model_dt.fit(X_train, y_train)\n",
        "fit['Dec Tree R'] = model_dt.score(X_test, y_test)\n",
        "\n",
        "\n",
        "# DECISION TREE-BASED ENSEMBLE MODELS: great for minimizing overfitting, these are based on averaging many unique sub-samples and combining algorithms \n",
        "# 1.17. Decision Forrest\n",
        "model_df = se.RandomForestRegressor(random_state=12345)\n",
        "model_df.fit(X_train, y_train)\n",
        "fit['Dec Forest R'] = model_df.score(X_test, y_test)\n",
        "\n",
        "# 1.18. ExtraTreesRegressor\n",
        "model_etr = se.ExtraTreesRegressor(random_state=12345)\n",
        "model_etr.fit(X_train, y_train)\n",
        "fit['Extra Trees R'] = model_etr.score(X_test, y_test)\n",
        "\n",
        "# 1.19. AdaBoostRegressor\n",
        "model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=12345)\n",
        "model_abr.fit(X_train, y_train)\n",
        "fit['AdaBoost DT R'] = model_abr.score(X_test, y_test)\n",
        "\n",
        "# 1.20. GradientBoostingRegressor\n",
        "model_gbr = se.GradientBoostingRegressor(random_state=12345)\n",
        "model_gbr.fit(X_train, y_train)\n",
        "fit['Grad. Boost R'] = model_gbr.score(X_test, y_test)\n",
        "\n",
        "# Only available in sklearn 0.23\n",
        "# # 1.21. HistGradientBoostingRegressor\n",
        "# model_hgbr = se.HistGradientBoostingRegressor(random_state=12345)\n",
        "# model_hgbr.fit(X_train, y_train)\n",
        "# fit['HG Boost R'] = model_hgbr.score(X_test, y_test)\n",
        "\n",
        "# 1.22. VotingRegressor: will combine other algorithms into an average; kind of cool\n",
        "model_vr = se.VotingRegressor(estimators=[('DT', model_dt), ('DF', model_df), ('ETR', model_etr), ('ABR', model_abr), ('GBR', model_gbr)])\n",
        "model_vr.fit(X_train, y_train)\n",
        "fit['Voting R'] = model_vr.score(X_test, y_test)\n",
        "\n",
        "# 1.23. StackingRegressor\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "estimators = [('ridge', RidgeCV()), ('lasso', LassoCV(random_state=42)), ('svr', svm.SVR(C=1, gamma=1e-6))]\n",
        "model_sr = se.StackingRegressor(estimators=estimators, final_estimator=se.GradientBoostingRegressor(random_state=12345))\n",
        "model_sr.fit(X_train, y_train)\n",
        "fit['Stacking R'] = model_sr.score(X_test, y_test)\n",
        "                                       \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# NEURAL-NETWORK MODELS: Based on deep learning methods\n",
        "import sklearn.neural_network as nn\n",
        "\n",
        "# 1.24. MLPRegressor\n",
        "model_nn = nn.MLPRegressor(max_iter=1000, random_state=12345) # Turn max_iter way up or down to get a more accurate result\n",
        "model_nn.fit(X_train, y_train)\n",
        "fit['NeuralNet R'] = model_nn.score(X_test, y_test)\n",
        "\n",
        "\n",
        "# Sort and print the dictionary by greatest R squared to least\n",
        "r2s = sorted_list_by_value=sorted(fit, key=fit.__getitem__, reverse=True)\n",
        "for r2 in r2s:\n",
        "  print(f'{r2}:\\t{fit[r2]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grad. Boost R:\t0.8650458515184248\n",
            "Voting R:\t0.8519465154506137\n",
            "AdaBoost DT R:\t0.8388511111248289\n",
            "Dec Forest R:\t0.8385632868961774\n",
            "Extra Trees R:\t0.8279342724648359\n",
            "KNNeighborsD R:\t0.7855833812136057\n",
            "KNNeighbors R:\t0.768085491728174\n",
            "Stacking R:\t0.7627918843997207\n",
            "Ridge R:\t0.7538916925024396\n",
            "Bayesian R:\t0.7538302032093211\n",
            "LARS Lasso R:\t0.7537894054354863\n",
            "Lasso R:\t0.753693283731631\n",
            "OrdinaryLS R:\t0.753686352357909\n",
            "SGradientD R:\t0.7514811096049173\n",
            "Dec Tree R:\t0.721206134184744\n",
            "NeuralNet R:\t0.06454413627444944\n",
            "GaussianP R:\t0.0014095746981229729\n",
            "NuSupportVM R:\t-0.02645764932270156\n",
            "SupportVM R:\t-0.08468368331295606\n",
            "Linear SVM R:\t-0.8793004634454971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trYmj53pZrQi"
      },
      "source": [
        "## Choose the Best Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqMgiQiE0r0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec4063c-3156-4d56-8209-807535befc62"
      },
      "source": [
        "# Select the model with the highest R squared\n",
        "print(f'Best model: {r2s[0]} (R2:{fit[r2s[0]]})')\n",
        "model = fit[r2s[1]]\n",
        "type(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best model: Grad. Boost R (R2:0.8650458515184248)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1tKKzP9agOu"
      },
      "source": [
        "## Save Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ads2UVlIae6E",
        "outputId": "a59563dd-6673-4e04-b5f9-f7f1d62a0f60"
      },
      "source": [
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# Save the model with the highest fit metric\n",
        "pickle.dump(model, open('stored_model.sav', 'wb'))  # OPTION 1: pickle\n",
        "joblib.dump(model, \"stored_model.pkl\")              # OPTION 2: joblib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['stored_model.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMhvvyrGx13H"
      },
      "source": [
        "### Pickle Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZe7R20GpU0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "19ac1eed-4a3b-4662-81f3-9c2b96de6555"
      },
      "source": [
        "# ...some time later\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# OPTION 1: Using pickle\n",
        "# load the model from 'stored_model.sav'\n",
        "loaded_model = pickle.load(open('stored_model.sav', 'rb'))\n",
        "print(type(loaded_model))\n",
        "\n",
        "# for a single prediction, enter a row of data and reshape into numpy array\n",
        "case = [0.543478, 0.245359, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
        "print(f'Single prediction {case}: {loaded_model.predict(np.array(case).reshape(1, -1))[0]}\\n')\n",
        "\n",
        "# for a batch prediction, enter a Pandas DataFrame or a Numpy array of arrays\n",
        "predictions = loaded_model.predict(X_test) \n",
        "batch_results = pd.DataFrame({'Actual':y_test, 'Predicted':predictions, 'Diff':(predictions - y_test)})\n",
        "print(f'MAE:\\t{batch_results.Diff.abs().mean()}\\n')\n",
        "batch_results.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.float64'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0b5b1f94b97e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# for a single prediction, enter a row of data and reshape into numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.543478\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.245359\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Single prediction {case}: {loaded_model.predict(np.array(case).reshape(1, -1))[0]}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# for a batch prediction, enter a Pandas DataFrame or a Numpy array of arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'predict'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4a-NS_Zzrm"
      },
      "source": [
        "### Joblib Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TA0vdSCsO4U"
      },
      "source": [
        "# OPTION 2: Using joblib\n",
        "from sklearn.externals import joblib\n",
        "classifer = joblib.load(\"stored_model.pkl\")\n",
        "\n",
        "# for a single prediction, enter a row of data and reshape into numpy array\n",
        "case = [0.543478, 0.245359, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]\n",
        "print(f'Single prediction {case}: {loaded_model.predict(np.array(case).reshape(1, -1))[0]}\\n')\n",
        "\n",
        "# for a batch prediction, enter a Pandas DataFrame or a Numpy array of arrays\n",
        "predictions = loaded_model.predict(X_test) \n",
        "batch_results = pd.DataFrame({'Actual':y_test, 'Predicted':predictions, 'Diff':(predictions - y_test)})\n",
        "print(f'MAE:\\t{batch_results.Diff.abs().mean()}\\n')\n",
        "batch_results.tail(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErwUNZfZcScj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}