{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Midterm.ipynb","provenance":[],"authorship_tag":"ABX9TyOsfY15gYxx908lXsLuGzpo"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"RC1F3g8slnmW"},"source":["# Question 0: \n","# ^^^ Leave these lines this like this without modifying (for the autograder). Do not add any more text to the \n","# 'Question #:' line. Make sure that all code needed to answer each question is found within the designated code\n","# cell. If you want to use one of your own functions, copy the function into the code cell where it is needed."],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"5aQ1_KqGlscb","executionInfo":{"status":"ok","timestamp":1603139990836,"user_tz":360,"elapsed":1158,"user":{"displayName":"Mark Keith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiISgHTD0uohYa-QGHVHJt9uoH8LFCBfx2XJmIl6A=s64","userId":"01471376858192886314"}},"outputId":"d47d891d-643a-42c7-f766-6bfe0b20c7fe","colab":{"base_uri":"https://localhost:8080/","height":634}},"source":["# Question 1: \n","# Import the dataset 'lc_20k_no_missing.csv' into a DataFrame\n","# Print the number of rows and columns (+1)\n","# Print the first first 10 rows (+1)\n","import pandas as pd \n","\n","df = pd.read_csv('..\\lc_20k_no_missing.csv')\n","print(df.shape)\n","df.head(10)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(20000, 35)\n"]},{"output_type":"execute_result","data":{"text/plain":["  loan_status  loan_status_numeric  loan_amnt  ...    dti grade  sub_grade\n","0     Current                    5      20800  ...  21.82     C         C4\n","1  Fully Paid                    6      16000  ...  15.31     C         C5\n","2     Current                    5      12000  ...  15.69     C         C3\n","3     Current                    5      15000  ...   9.10     C         C1\n","4  Fully Paid                    6      35000  ...  24.90     A         A5\n","5  Fully Paid                    6       6050  ...  12.78     A         A3\n","6     Current                    5      10000  ...  11.13     A         A1\n","7     Current                    5       8500  ...  12.69     A         A3\n","8  Fully Paid                    6      20000  ...  14.06     D         D2\n","9  Fully Paid                    6      24000  ...  13.64     D         D3\n","\n","[10 rows x 35 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loan_status</th>\n      <th>loan_status_numeric</th>\n      <th>loan_amnt</th>\n      <th>issue_d</th>\n      <th>term</th>\n      <th>int_rate</th>\n      <th>installment</th>\n      <th>total_pymnt</th>\n      <th>total_rec_prncp</th>\n      <th>total_rec_int</th>\n      <th>total_rec_late_fee</th>\n      <th>title</th>\n      <th>purpose</th>\n      <th>emp_title</th>\n      <th>emp_length</th>\n      <th>home_ownership</th>\n      <th>annual_inc</th>\n      <th>verification_status</th>\n      <th>acc_now_delinq</th>\n      <th>delinq_2yrs</th>\n      <th>earliest_cr_line</th>\n      <th>inq_last_6mths</th>\n      <th>mths_since_last_delinq</th>\n      <th>mths_since_last_record</th>\n      <th>open_acc</th>\n      <th>pub_rec</th>\n      <th>revol_bal</th>\n      <th>revol_util</th>\n      <th>tot_coll_amt</th>\n      <th>tot_cur_bal</th>\n      <th>total_acc</th>\n      <th>total_rev_hi_lim</th>\n      <th>dti</th>\n      <th>grade</th>\n      <th>sub_grade</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Current</td>\n      <td>5</td>\n      <td>20800</td>\n      <td>2/1/2017</td>\n      <td>36 months</td>\n      <td>14.99</td>\n      <td>720.94</td>\n      <td>17285.240000</td>\n      <td>12812.18</td>\n      <td>4473.06</td>\n      <td>0.0</td>\n      <td>Debt consolidation</td>\n      <td>debt_consolidation</td>\n      <td>Service Coordinator</td>\n      <td>10+ years</td>\n      <td>RENT</td>\n      <td>47308.00</td>\n      <td>Verified</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9/1/1999</td>\n      <td>2</td>\n      <td>85</td>\n      <td>55</td>\n      <td>11</td>\n      <td>1</td>\n      <td>11084</td>\n      <td>32.3</td>\n      <td>0</td>\n      <td>30479</td>\n      <td>21</td>\n      <td>34300</td>\n      <td>21.82</td>\n      <td>C</td>\n      <td>C4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Fully Paid</td>\n      <td>6</td>\n      <td>16000</td>\n      <td>9/1/2016</td>\n      <td>36 months</td>\n      <td>15.59</td>\n      <td>559.28</td>\n      <td>19366.367420</td>\n      <td>16000.00</td>\n      <td>3366.37</td>\n      <td>0.0</td>\n      <td>Debt consolidation</td>\n      <td>debt_consolidation</td>\n      <td>Buyer</td>\n      <td>10+ years</td>\n      <td>RENT</td>\n      <td>65000.00</td>\n      <td>Verified</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1/1/2004</td>\n      <td>0</td>\n      <td>33</td>\n      <td>34</td>\n      <td>11</td>\n      <td>1</td>\n      <td>4112</td>\n      <td>54.1</td>\n      <td>0</td>\n      <td>29339</td>\n      <td>25</td>\n      <td>7600</td>\n      <td>15.31</td>\n      <td>C</td>\n      <td>C5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Current</td>\n      <td>5</td>\n      <td>12000</td>\n      <td>5/1/2018</td>\n      <td>36 months</td>\n      <td>14.52</td>\n      <td>413.17</td>\n      <td>3631.410000</td>\n      <td>2531.81</td>\n      <td>1099.60</td>\n      <td>0.0</td>\n      <td>Debt consolidation</td>\n      <td>debt_consolidation</td>\n      <td>MANAGER</td>\n      <td>10+ years</td>\n      <td>MORTGAGE</td>\n      <td>105000.00</td>\n      <td>Source Verified</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5/1/1998</td>\n      <td>0</td>\n      <td>85</td>\n      <td>49</td>\n      <td>18</td>\n      <td>1</td>\n      <td>13915</td>\n      <td>47.0</td>\n      <td>310</td>\n      <td>338073</td>\n      <td>32</td>\n      <td>29600</td>\n      <td>15.69</td>\n      <td>C</td>\n      <td>C3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Current</td>\n      <td>5</td>\n      <td>15000</td>\n      <td>9/1/2017</td>\n      <td>60 months</td>\n      <td>12.62</td>\n      <td>338.39</td>\n      <td>5450.360000</td>\n      <td>3128.99</td>\n      <td>2321.37</td>\n      <td>0.0</td>\n      <td>Debt consolidation</td>\n      <td>debt_consolidation</td>\n      <td>teacher</td>\n      <td>&lt; 1 year</td>\n      <td>MORTGAGE</td>\n      <td>65000.00</td>\n      <td>Not Verified</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7/1/1998</td>\n      <td>0</td>\n      <td>22</td>\n      <td>85</td>\n      <td>9</td>\n      <td>0</td>\n      <td>14827</td>\n      <td>32.9</td>\n      <td>0</td>\n      <td>704898</td>\n      <td>17</td>\n      <td>45000</td>\n      <td>9.10</td>\n      <td>C</td>\n      <td>C1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Fully Paid</td>\n      <td>6</td>\n      <td>35000</td>\n      <td>5/1/2017</td>\n      <td>36 months</td>\n      <td>7.97</td>\n      <td>1096.29</td>\n      <td>37852.531640</td>\n      <td>35000.00</td>\n      <td>2852.53</td>\n      <td>0.0</td>\n      <td>Credit card refinancing</td>\n      <td>credit_card</td>\n      <td>Director, Quality Management</td>\n      <td>&lt; 1 year</td>\n      <td>MORTGAGE</td>\n      <td>139200.00</td>\n      <td>Source Verified</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1/1/1998</td>\n      <td>0</td>\n      <td>85</td>\n      <td>85</td>\n      <td>10</td>\n      <td>0</td>\n      <td>63912</td>\n      <td>64.0</td>\n      <td>0</td>\n      <td>258089</td>\n      <td>19</td>\n      <td>86500</td>\n      <td>24.90</td>\n      <td>A</td>\n      <td>A5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Fully Paid</td>\n      <td>6</td>\n      <td>6050</td>\n      <td>2/1/2017</td>\n      <td>36 months</td>\n      <td>7.24</td>\n      <td>187.48</td>\n      <td>6462.363708</td>\n      <td>6050.00</td>\n      <td>412.36</td>\n      <td>0.0</td>\n      <td>Debt consolidation</td>\n      <td>debt_consolidation</td>\n      <td>Assembly/ packaging  l√≠ne operator</td>\n      <td>1 year</td>\n      <td>MORTGAGE</td>\n      <td>18976.83</td>\n      <td>Verified</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3/1/2004</td>\n      <td>0</td>\n      <td>74</td>\n      <td>85</td>\n      <td>6</td>\n      <td>0</td>\n      <td>11278</td>\n      <td>41.2</td>\n      <td>0</td>\n      <td>114887</td>\n      <td>30</td>\n      <td>27400</td>\n      <td>12.78</td>\n      <td>A</td>\n      <td>A3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Current</td>\n      <td>5</td>\n      <td>10000</td>\n      <td>7/1/2017</td>\n      <td>36 months</td>\n      <td>5.32</td>\n      <td>301.15</td>\n      <td>5718.890000</td>\n      <td>5079.19</td>\n      <td>639.70</td>\n      <td>0.0</td>\n      <td>Debt consolidation</td>\n      <td>debt_consolidation</td>\n      <td>Psychiatric Social Worker II</td>\n      <td>10+ years</td>\n      <td>RENT</td>\n      <td>80000.00</td>\n      <td>Not Verified</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10/1/1999</td>\n      <td>0</td>\n      <td>85</td>\n      <td>85</td>\n      <td>8</td>\n      <td>0</td>\n      <td>6098</td>\n      <td>17.2</td>\n      <td>0</td>\n      <td>56732</td>\n      <td>12</td>\n      <td>35500</td>\n      <td>11.13</td>\n      <td>A</td>\n      <td>A1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Current</td>\n      <td>5</td>\n      <td>8500</td>\n      <td>10/1/2017</td>\n      <td>36 months</td>\n      <td>7.21</td>\n      <td>263.28</td>\n      <td>4287.930000</td>\n      <td>3642.65</td>\n      <td>645.28</td>\n      <td>0.0</td>\n      <td>Credit card refinancing</td>\n      <td>credit_card</td>\n      <td>General Sales Manager</td>\n      <td>2 years</td>\n      <td>RENT</td>\n      <td>87000.00</td>\n      <td>Source Verified</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10/1/2009</td>\n      <td>1</td>\n      <td>70</td>\n      <td>85</td>\n      <td>16</td>\n      <td>0</td>\n      <td>2883</td>\n      <td>7.8</td>\n      <td>0</td>\n      <td>12587</td>\n      <td>23</td>\n      <td>36900</td>\n      <td>12.69</td>\n      <td>A</td>\n      <td>A3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Fully Paid</td>\n      <td>6</td>\n      <td>20000</td>\n      <td>7/1/2018</td>\n      <td>36 months</td>\n      <td>18.94</td>\n      <td>732.52</td>\n      <td>21384.589440</td>\n      <td>20000.00</td>\n      <td>1384.59</td>\n      <td>0.0</td>\n      <td>Car financing</td>\n      <td>car</td>\n      <td>PC</td>\n      <td>1 year</td>\n      <td>RENT</td>\n      <td>35000.00</td>\n      <td>Verified</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3/1/2009</td>\n      <td>1</td>\n      <td>80</td>\n      <td>85</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2338</td>\n      <td>36.0</td>\n      <td>0</td>\n      <td>9242</td>\n      <td>6</td>\n      <td>6500</td>\n      <td>14.06</td>\n      <td>D</td>\n      <td>D2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Fully Paid</td>\n      <td>6</td>\n      <td>24000</td>\n      <td>12/1/2017</td>\n      <td>60 months</td>\n      <td>19.03</td>\n      <td>622.97</td>\n      <td>27713.986180</td>\n      <td>24000.00</td>\n      <td>3713.99</td>\n      <td>0.0</td>\n      <td>Credit card refinancing</td>\n      <td>credit_card</td>\n      <td>Plant Manager</td>\n      <td>10+ years</td>\n      <td>MORTGAGE</td>\n      <td>140000.00</td>\n      <td>Source Verified</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1/1/2001</td>\n      <td>1</td>\n      <td>31</td>\n      <td>85</td>\n      <td>18</td>\n      <td>0</td>\n      <td>14375</td>\n      <td>54.0</td>\n      <td>0</td>\n      <td>349441</td>\n      <td>44</td>\n      <td>26600</td>\n      <td>13.64</td>\n      <td>D</td>\n      <td>D3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"9yLZFBCslsfN","executionInfo":{"status":"ok","timestamp":1603139996635,"user_tz":360,"elapsed":766,"user":{"displayName":"Mark Keith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiISgHTD0uohYa-QGHVHJt9uoH8LFCBfx2XJmIl6A=s64","userId":"01471376858192886314"}},"outputId":"957a3d3d-36cc-4928-998c-ebb551d4ee10","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Question 2:\n","# Run all univariate statistics and metrics on the dataset and print them out a DataFrame. (+5)\n","# How many features have missing data?\n","\n","import pandas as pd\n","\n","df_output = pd.DataFrame(columns=['Count', 'Missing', 'Min', 'Q1', 'Q2/Median', 'Q3', 'Max', 'Mode', 'Mean', 'Std', 'Skew', 'Kurtosis'])\n","for col in df: #loops through each column in the dataframe\n","  if pd.api.types.is_numeric_dtype(df[col].dtype): #shows whether the data type is numeric or nahh\n","    df_output.loc[col] = [df[col].count(), df[col].isnull().sum(), df[col].min(), df[col].quantile(.25), df[col].median(), df[col].quantile(.75), df[col].max(), df[col].mode().values[0], df[col].mean(), df[col].std(), df[col].skew(), df[col].kurt()]    \n","  else: #not numeric\n","    df_output.loc[col] = [df[col].count(), df[col].isnull().sum(), '-', '-', '-', '-', '-', df[col].mode().values[0], '-', '-', '-', '-']    \n","    \n","df_output"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                        Count Missing    Min  ...       Std      Skew  Kurtosis\n","loan_status             20000       0      -  ...         -         -         -\n","loan_status_numeric     20000       0      0  ...   1.51193  -2.45869   5.20993\n","loan_amnt               20000       0   1000  ...   9776.78  0.786712 -0.209633\n","issue_d                 20000       0      -  ...         -         -         -\n","term                    20000       0      -  ...         -         -         -\n","int_rate                20000       0   5.31  ...   5.13088  0.943187  0.907684\n","installment             20000       0  30.12  ...   285.197   1.00167  0.516908\n","total_pymnt             20000       0      0  ...   8754.78   1.65583   2.75681\n","total_rec_prncp         20000       0      0  ...   7718.84   1.84546   3.41639\n","total_rec_int           20000       0      0  ...   2140.92   2.56607   9.72287\n","total_rec_late_fee      20000       0      0  ...   11.8385   21.1339   713.945\n","title                   20000       0      -  ...         -         -         -\n","purpose                 20000       0      -  ...         -         -         -\n","emp_title               20000       0      -  ...         -         -         -\n","emp_length              20000       0      -  ...         -         -         -\n","home_ownership          20000       0      -  ...         -         -         -\n","annual_inc              20000       0   1800  ...   67195.4   14.6907   588.117\n","verification_status     20000       0      -  ...         -         -         -\n","acc_now_delinq          20000       0      0  ...  0.063914   17.0548   310.534\n","delinq_2yrs             20000       0      0  ...  0.859672   6.40334   76.9194\n","earliest_cr_line        20000       0      -  ...         -         -         -\n","inq_last_6mths          20000       0      0  ...  0.788941   1.79616   3.56101\n","mths_since_last_delinq  20000       0      0  ...   29.3477 -0.645604  -1.19319\n","mths_since_last_record  20000       0      1  ...   10.7055  -3.35936   17.6819\n","open_acc                20000       0      1  ...   5.85609   1.33281   3.40065\n","pub_rec                 20000       0      0  ...  0.523387   4.55676   35.9789\n","revol_bal               20000       0      0  ...   23749.8   10.7142   275.416\n","revol_util              20000       0      0  ...   24.6823  0.108021 -0.816159\n","tot_coll_amt            20000       0      0  ...   1976.35   45.8835   3354.18\n","tot_cur_bal             20000       0      0  ...    165208   2.25783   9.77403\n","total_acc               20000       0      2  ...   12.0182   1.11025   2.30186\n","total_rev_hi_lim        20000       0    100  ...   36393.5   5.72395   95.8747\n","dti                     20000       0      0  ...   11.1893   7.28392   212.304\n","grade                   20000       0      -  ...         -         -         -\n","sub_grade               20000       0      -  ...         -         -         -\n","\n","[35 rows x 12 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Count</th>\n      <th>Missing</th>\n      <th>Min</th>\n      <th>Q1</th>\n      <th>Q2/Median</th>\n      <th>Q3</th>\n      <th>Max</th>\n      <th>Mode</th>\n      <th>Mean</th>\n      <th>Std</th>\n      <th>Skew</th>\n      <th>Kurtosis</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>loan_status</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>Current</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>loan_status_numeric</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>6</td>\n      <td>6</td>\n      <td>5</td>\n      <td>4.82935</td>\n      <td>1.51193</td>\n      <td>-2.45869</td>\n      <td>5.20993</td>\n    </tr>\n    <tr>\n      <th>loan_amnt</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>1000</td>\n      <td>8000</td>\n      <td>13000</td>\n      <td>20381.2</td>\n      <td>40000</td>\n      <td>10000</td>\n      <td>15441.3</td>\n      <td>9776.78</td>\n      <td>0.786712</td>\n      <td>-0.209633</td>\n    </tr>\n    <tr>\n      <th>issue_d</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>3/1/2016</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>term</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>36 months</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>int_rate</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>5.31</td>\n      <td>9.44</td>\n      <td>11.99</td>\n      <td>15.99</td>\n      <td>30.99</td>\n      <td>11.99</td>\n      <td>13.0054</td>\n      <td>5.13088</td>\n      <td>0.943187</td>\n      <td>0.907684</td>\n    </tr>\n    <tr>\n      <th>installment</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>30.12</td>\n      <td>249.868</td>\n      <td>378.2</td>\n      <td>611.67</td>\n      <td>1566.59</td>\n      <td>361.38</td>\n      <td>456.856</td>\n      <td>285.197</td>\n      <td>1.00167</td>\n      <td>0.516908</td>\n    </tr>\n    <tr>\n      <th>total_pymnt</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3112.8</td>\n      <td>6498.68</td>\n      <td>12615.1</td>\n      <td>55108</td>\n      <td>0</td>\n      <td>9364.26</td>\n      <td>8754.78</td>\n      <td>1.65583</td>\n      <td>2.75681</td>\n    </tr>\n    <tr>\n      <th>total_rec_prncp</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>4575.02</td>\n      <td>10000</td>\n      <td>40000</td>\n      <td>10000</td>\n      <td>7334.94</td>\n      <td>7718.84</td>\n      <td>1.84546</td>\n      <td>3.41639</td>\n    </tr>\n    <tr>\n      <th>total_rec_int</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>568.163</td>\n      <td>1220.93</td>\n      <td>2518.09</td>\n      <td>24058.2</td>\n      <td>0</td>\n      <td>1945.19</td>\n      <td>2140.92</td>\n      <td>2.56607</td>\n      <td>9.72287</td>\n    </tr>\n    <tr>\n      <th>total_rec_late_fee</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>623.8</td>\n      <td>0</td>\n      <td>1.38792</td>\n      <td>11.8385</td>\n      <td>21.1339</td>\n      <td>713.945</td>\n    </tr>\n    <tr>\n      <th>title</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>Debt consolidation</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>purpose</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>debt_consolidation</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>emp_title</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>Manager</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>emp_length</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>10+ years</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>home_ownership</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>MORTGAGE</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>annual_inc</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>1800</td>\n      <td>49500</td>\n      <td>69000</td>\n      <td>98000</td>\n      <td>3.76064e+06</td>\n      <td>60000</td>\n      <td>82072.1</td>\n      <td>67195.4</td>\n      <td>14.6907</td>\n      <td>588.117</td>\n    </tr>\n    <tr>\n      <th>verification_status</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>Source Verified</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>acc_now_delinq</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.0039</td>\n      <td>0.063914</td>\n      <td>17.0548</td>\n      <td>310.534</td>\n    </tr>\n    <tr>\n      <th>delinq_2yrs</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21</td>\n      <td>0</td>\n      <td>0.29925</td>\n      <td>0.859672</td>\n      <td>6.40334</td>\n      <td>76.9194</td>\n    </tr>\n    <tr>\n      <th>earliest_cr_line</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>8/1/2006</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>inq_last_6mths</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0.4984</td>\n      <td>0.788941</td>\n      <td>1.79616</td>\n      <td>3.56101</td>\n    </tr>\n    <tr>\n      <th>mths_since_last_delinq</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>33</td>\n      <td>85</td>\n      <td>85</td>\n      <td>168</td>\n      <td>85</td>\n      <td>60.4799</td>\n      <td>29.3477</td>\n      <td>-0.645604</td>\n      <td>-1.19319</td>\n    </tr>\n    <tr>\n      <th>mths_since_last_record</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>85</td>\n      <td>85</td>\n      <td>85</td>\n      <td>120</td>\n      <td>85</td>\n      <td>83.2721</td>\n      <td>10.7055</td>\n      <td>-3.35936</td>\n      <td>17.6819</td>\n    </tr>\n    <tr>\n      <th>open_acc</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>8</td>\n      <td>11</td>\n      <td>15</td>\n      <td>66</td>\n      <td>9</td>\n      <td>11.83</td>\n      <td>5.85609</td>\n      <td>1.33281</td>\n      <td>3.40065</td>\n    </tr>\n    <tr>\n      <th>pub_rec</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0.19165</td>\n      <td>0.523387</td>\n      <td>4.55676</td>\n      <td>35.9789</td>\n    </tr>\n    <tr>\n      <th>revol_bal</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5811.75</td>\n      <td>11218.5</td>\n      <td>20406</td>\n      <td>1.03901e+06</td>\n      <td>0</td>\n      <td>16956.8</td>\n      <td>23749.8</td>\n      <td>10.7142</td>\n      <td>275.416</td>\n    </tr>\n    <tr>\n      <th>revol_util</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>28.5</td>\n      <td>46.5</td>\n      <td>66.4</td>\n      <td>162.1</td>\n      <td>0</td>\n      <td>47.4915</td>\n      <td>24.6823</td>\n      <td>0.108021</td>\n      <td>-0.816159</td>\n    </tr>\n    <tr>\n      <th>tot_coll_amt</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>173380</td>\n      <td>0</td>\n      <td>226.087</td>\n      <td>1976.35</td>\n      <td>45.8835</td>\n      <td>3354.18</td>\n    </tr>\n    <tr>\n      <th>tot_cur_bal</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31017.2</td>\n      <td>85740</td>\n      <td>225010</td>\n      <td>2.26068e+06</td>\n      <td>0</td>\n      <td>150296</td>\n      <td>165208</td>\n      <td>2.25783</td>\n      <td>9.77403</td>\n    </tr>\n    <tr>\n      <th>total_acc</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>2</td>\n      <td>15</td>\n      <td>22</td>\n      <td>30</td>\n      <td>113</td>\n      <td>20</td>\n      <td>23.6316</td>\n      <td>12.0182</td>\n      <td>1.11025</td>\n      <td>2.30186</td>\n    </tr>\n    <tr>\n      <th>total_rev_hi_lim</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>100</td>\n      <td>15600</td>\n      <td>27200</td>\n      <td>45800</td>\n      <td>1.2525e+06</td>\n      <td>10000</td>\n      <td>36652.9</td>\n      <td>36393.5</td>\n      <td>5.72395</td>\n      <td>95.8747</td>\n    </tr>\n    <tr>\n      <th>dti</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11.95</td>\n      <td>17.97</td>\n      <td>24.84</td>\n      <td>462.67</td>\n      <td>21.6</td>\n      <td>18.9948</td>\n      <td>11.1893</td>\n      <td>7.28392</td>\n      <td>212.304</td>\n    </tr>\n    <tr>\n      <th>grade</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>B</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>sub_grade</th>\n      <td>20000</td>\n      <td>0</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>B5</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"Hc3jC_2ilshb","executionInfo":{"status":"ok","timestamp":1603140005565,"user_tz":360,"elapsed":338,"user":{"displayName":"Mark Keith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiISgHTD0uohYa-QGHVHJt9uoH8LFCBfx2XJmIl6A=s64","userId":"01471376858192886314"}},"outputId":"f8b9fd2a-fed5-463e-9412-cb487701b285","colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["# Question 3:\n","# Which categorical feature¬†is most¬†likely to have groups with fewer than 5% of the cases?\n","# Eliminate¬†that¬†feature¬†identified¬†in¬†the¬†prior¬†step¬†that¬†have¬†too¬†many¬†unique¬†categorical¬†groups.¬†Do¬†\n","# this¬†on¬†the¬†original¬†DataFrame¬†(not¬†on¬†a¬†copy¬†of¬†the¬†DataFrame)¬†using¬†the¬†inplace¬†attribute¬†(+1). \n","# Print out the first five records of the original DataFrame to make sure that feature was deleted.\n","\n","  from scipy import stats\n","  import pandas as pd\n","  import numpy as np\n","\n","  # Create an empty DataFrame to store output\n","  output_df = pd.DataFrame(columns=['Stat', '+/-', 'Effect size', 'p-value'])\n","\n","  for col in df:\n","    if not col == label:\n","      if df[col].isnull().sum() == 0:\n","        if pd.api.types.is_numeric_dtype(df[label]):\n","          if pd.api.types.is_numeric_dtype(df[col]): # Only calculate r, p-value for the numeric columns\n","            r, p = stats.pearsonr(df[label], df[col])\n","            output_df.loc[col] = ['r', np.sign(r), abs(round(r, 3)), round(p, 6)]\n","            scatter(df[col], df[label])\n","          else:\n","            F, p = anova(df[[col, label]], col, label)\n","            output_df.loc[col] = ['F', '', round(F, 3), round(p, 6)]\n","            bar_chart(df, col, label)\n","        else:\n","          pearsonchisquare(df[col], df[label])\n","      else:\n","        output_df.loc[col] = [np.nan, np.nan, np.nan, np.nan]\n","  \n","\n","output_df.head()"],"execution_count":2,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"unexpected indent (<ipython-input-2-76609fe0ad76>, line 7)","traceback":["\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-76609fe0ad76>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    from scipy import stats\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"]}]},{"cell_type":"code","metadata":{"id":"nQDy4MTblsj9","executionInfo":{"status":"ok","timestamp":1603140012484,"user_tz":360,"elapsed":355,"user":{"displayName":"Mark Keith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiISgHTD0uohYa-QGHVHJt9uoH8LFCBfx2XJmIl6A=s64","userId":"01471376858192886314"}},"outputId":"75c33dda-f809-4ce0-bcbf-d02f49398867","colab":{"base_uri":"https://localhost:8080/","height":765}},"source":["# Question 4:\n","# sub_grade appears to be a more detailed version of grade. Which should we keep? To determine this, \n","# write a routine or expression to count the number of unique values of grade and sub_grade (+2). You \n","# might find the DataFrame method .value_counts() useful for this task.\n","# Which feature(s) has groups representing less than 5% of the dataset?\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDuXIqcGlsmC","executionInfo":{"status":"ok","timestamp":1603140023528,"user_tz":360,"elapsed":392,"user":{"displayName":"Mark Keith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiISgHTD0uohYa-QGHVHJt9uoH8LFCBfx2XJmIl6A=s64","userId":"01471376858192886314"}},"outputId":"f91b8e6e-227e-44e9-f7ed-a02e1d61f3fd","colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["# Question 5:\n","# Delete only the one feature (from the original DataFrame) from the prior question that \n","# has the most groups representing  less than 5% of the overall number of cases (+1).\n","# Print the first 5 records of the DataFrame to verify that it is deleted.\n","# Which feature did you delete?\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"YgAMf0a3lsoc","executionInfo":{"status":"ok","timestamp":1603140032242,"user_tz":360,"elapsed":374,"user":{"displayName":"Mark Keith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiISgHTD0uohYa-QGHVHJt9uoH8LFCBfx2XJmIl6A=s64","userId":"01471376858192886314"}},"outputId":"79a551b1-4cc3-4221-c531-9ddbd5d5a479","colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["# Question 6:\n","# Examine the univariate properties generated in a prior step. Which features have skewness problems assuming \n","# that you round to two decimal places? Answer this question below. You must select every correct answer and no \n","# incorrect answers to receive full credit.\n","# \n","# Write a routine that will print only the names and skewness scores of those features that voliate the skewness \n","# assumption in order from most to least skewness (+2). \n","# \n","# HINT: It is okay to change the sign of the skewness score to make all of them comparable. Also, filtering a \n","# Pandas Series is much like filtering a DataFrame except that you don't refer to the feature name. e.g.:\n","\n","# print(series[series > num]) # where num¬†refers to your skewness cutoff\n","\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"dXrs8Gfklsq7","executionInfo":{"status":"ok","timestamp":1603140042708,"user_tz":360,"elapsed":371,"user":{"displayName":"Mark Keith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiISgHTD0uohYa-QGHVHJt9uoH8LFCBfx2XJmIl6A=s64","userId":"01471376858192886314"}}},"source":["# Question 7:\n","# We need to determine which features may have heteroscedasticity problems with the label loan_status_numeric \n","# before we run the model. Check the heteroscedasticity of each numeric feature (floats and ints) with the label\n","# by using only White's Test. Place the results in a DataFrame with the following columns: feature name, \n","# Lagrange Multiplier (LM), LM p-value, F-stat, p-value. Sort the DataFrame F-stat to smallest. Round the results \n","# to three decimals. (+5)\n","\n","# Which feature has the most heteroscedasticity with loan_status_numeric based on White's F stat? \n","# Enter the name below exactly as it appears in the dataset. Capitalization doesn't matter.\n","\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"NsbwUZaZlstD","executionInfo":{"status":"ok","timestamp":1603140108587,"user_tz":360,"elapsed":352,"user":{"displayName":"Mark Keith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiISgHTD0uohYa-QGHVHJt9uoH8LFCBfx2XJmIl6A=s64","userId":"01471376858192886314"}},"outputId":"4e07c9d0-ce94-44a7-ccdf-70672ecc22d0","colab":{"base_uri":"https://localhost:8080/","height":833}},"source":["# Question 8:\n","# Create an MLR model to explain loan_status_numeric from all other numeric features in the dataset\n","# Do not convert any categorical features to numeric dummy codes for this task and be sure to allow\n","# the y-intercept to vary. Print the summary() method of the statsmodels OLS object after fitting \n","# the model (+3)\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"6oJ-67IDlsxt"},"source":["# Question 9:\n","# Print a list of the features and their coefficients. Sort the list from furthest from zero to closest. \n","# It is okay¬†to convert all negative coefficients to positive using an abosolute value if that helps. (+1)\n","\n","# Based on the coefficient estimate, what is the most important feature (i.e. highest coefficient) in the prior OLS?\n","\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Question 10:\n","# Print a list of the features and their t-test statistics. Sort the list from furthest from zero to closest. \n","# It is okay¬†to convert all negative t-stats to positive using an abosolute value if that helps. (+1)\n","\n","# Based on the t-test statistic, what is the most important feature in the prior OLS?\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Question 11:\n","# Next, we need to manage the date fields: issue_d and earliest_cr_line. One way to do this is to \n","# convert them to \"days/months/years since [date]\". Review chapter 4.7 (or whatever chapter \n","# \"Python: IPO\" is numbered as in your book) to see how to calculate the difference between dates. \n","# Or, find a suitable example on stackoverflow.com or similar (example¬†and example).\n","\n","# Create new versions of issue_d and earliest_cr_line that are calculated as the difference between \n","# the value and 12/31/2018 in months. Save these new features in place of the existing features. (+3)\n","# Another way to accomplish this is to create a new column with a default date of 12/31/2018. Then \n","# set issue_d = the difference between that new column and the existing issue_d column.\n","\n","# What is the maximum value of the resultling issue_d feature? Enter the days in integer form. For \n","# example, if the answer were '851¬†days 00:00:00', enter 851\n","\n","# FINALLY: if you struggle with this task and cannot complete it, just drop those two date fields \n","# (+1: this point an alternative to the 3 points above for converting. i.e. you can't get both 1 + 3 = 4)\n","\n"]},{"cell_type":"code","metadata":{"id":"mmTEK3Lhlsvg"},"source":["# Question: 12\n","# Let's test our model with all remaining variables included after completing each of \n","# the prior tasks. Create and run another OLS after creating dummy code values for all \n","# remaining categorical features. Continue to use loan_status_numeric as the label and \n","# be sure not to include loan_status in your model in any way. (+2)\n","\n","# Print the .summary() method of your fitted OLS object.\n","\n","# What is the R squared value of your model?¬†\n","\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"-aiQucmQlsz7"},"source":["# Question 13: \n","# Once again, based on the model created in the prior step, print out a list of \n","# features with t-tests stats and a separate list of features with coefficients.¬†(+1)\n","\n","# Based on the t-test stat, which feature is having the largest effect on \n","# loan_status_numeric?\n","\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Question 14: \n","# Next, scale the data to values ranging from 0 to 1. Run the OLS model again. Based on\n","# on the coefficient estimates, which feature has the largest effect?\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Question 15: \n","# Once again, print out a list of features and their coefficient values. Sort from largest from zero\n","# to smallest. It is okay to use absolute values. Also, print the features including their tvalues and \n","# sort from highest to lowest (again; it's okay to use absolute values).(+1)\n","\n","# Notice how the features toward the top of the coefficient list are much more similar to the list of\n","# those with the highest t-test statistics than when we ran this model without normalizing/scaling. \n","# This is one of the reasons we normalize. \n","\n","# What feature (not value) is at the top of the list of coefficient values that are furthest from zero?\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Question 16:\n","# Now let's check for assumptions. We have already checked for univariate skewness issues in the univariate \n","# statistics (although we haven't corrected the skewness and we won't for this midterm). However, let's now \n","# check overall model \"normalcy\". This has already been calculated in your last model. Which answer below\n","# represents the likelihood that the residuals are normally distributed.¬†\n","\n","# NOTE: There is no code to write for this question. Leave the corrosponding code cell as it is"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Question 17:\n","# Next, let's find out which features have too much multi-collinearity with the other features. Test each \n","# feature for multi-collinearity and print the results in a DataFrame (+2). Sort the DataFrame from largest \n","# to smallest multi-collinearity (+1).\n","\n","# To show all of the results, you may need to add the following line:\n","\n","# pd.set_option('display.max_rows',¬†500)\n","\n","# How many features have VIF scores above 10?\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Question 18:\n","# One-by-one, remove each feature with a VIF score greater than 10 until all features have VIF < 10. In other words:\n","\n","#    1. remove the one feature with the highest VIF\n","#    2. re-test VIF for all features.¬†\n","#    3. repeat until no more features have a VIF > 10\n","\n","# (+4)\n","\n","# NOTE: You do not have to write an automated loop to do this. You can simply remove the features \"manually\" each time. However, if you are going to automate this with a loop, you might find the .idxmax() method of DataFrames useful.\n","\n","# Print the remaining list of featues along with the VIF scores (Tolerance optional) (+1). \n","\n","# How many features were dropped?\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Question 19:\n","# Create one more model based on these remaining features after removing those with multi-\n","# collinearity issues. Your dataset should already have dummy codes and a MinMax scaling \n","# from the last model you created.¬†(+1). In other words, you could perform this step in \n","# only 1-2 lines of code.\n","\n","# Print out the .summary() method of the fitted OLS object.\n","\n","#¬†What is the R2 score?\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# Question 20:\n","# Write a routine that will help us identify the remaining features to keep in the model. \n","# To do this:\n","\n","#    1. Eliminate the one feature with the t-test statistic closest to zero\n","#    2. Store the R2, R2 adjusted, and the name of the feature with the tvalue closest to \n","#       zero¬†in a DataFrame with number of features as the row index\n","#    3. Re-run the MLR\n","#    4. Repeat Steps 1-3 until there is there are 5 features left in the model.¬†(+5)\n","#    5. Plot lines for R2 and R2 adjusted in a chart to visualize¬†(+3)\n","#          - Use the number of features in each model as the x-axis\n","#          - Use the R2 and R2 adjusted scores as the y-axis\n","#          - Adjust the y-axes to zoom in and maximize the difference between R2 and R2 adjusted\n","\n","# HINT: Looking for where to begin? You need a loop that will run (n - 5) times where n = the \n","# number of remaining features (not counting the label) and -5 is due to ending with 5 features \n","# left in the model. I've given you an example in the code below as well as some pseudocode to \n","# walk you through the rest. This is just one way to do this, you don't have to use this example:¬†\n","\n","#  1. Create an empty DataFrame with the columns 'R2', 'R2_adj', 'lowest t feat'; e.g. 'output_df'\n","#  2. Store your current features DataFrame in a new variable; e.g. 'features_df' (so you don't\n","#     have to keep re-running all of your prior code as you debug)\n","\n","#  3. for¬†i¬†in¬†range(len(features_df.columns)¬†-¬†5):\n","#  4.    Fit an OLS model using the label created previously and 'features_df'\n","# ¬†5.    Add a new row to output_df containing the R2, R2_adj, and the lowest tvalue feature\n","# ¬†      from the model you just fit in the line above\n","# ¬†6.    Drop that lowest tvalue feature from 'features_df'\n","\n","#  7. Create a lineplot including R2 and R2_adj\n","# ¬†8. Search Google/stackoverflow.com to see how to set the scale of the y-axes to highlight the\n","# ¬†   difference between R2 and R2_adj.\n","# ¬†9. Identify the point on the curve where R2 and R2_adj get as close together as possible \n","# ¬†   before taking a steep dive\n","\n","# About how many features (sorted from best to worst) should we include in our final model?\n","\n"]}]}