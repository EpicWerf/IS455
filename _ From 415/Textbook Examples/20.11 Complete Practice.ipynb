{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "MwS6-1EqIlq6",
        "outputId": "99190aa0-7b6f-4431-8ca4-b17c8fab9e8d"
      },
      "outputs": [],
      "source": [
        "def import_housing_data(url) :\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(url)\n",
        "    df.drop(columns=['Id'], inplace=True)\n",
        "    df.dropna(axis=1, inplace=True)\n",
        "\n",
        "    #this little guy finds any column name that starts with a number and ads a string in front of it so you don't have issues later\n",
        "    for col in df :\n",
        "      if col[0].isdigit():\n",
        "        nums = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
        "        df.rename(columns={col:nums[int(col[0])] + '_' + col}, inplace = True)\n",
        "    return df\n",
        "\n",
        "# For VSCode\n",
        "# import sys\n",
        "# sys.path.append('/Textbook Examples/')\n",
        "# import functions as fun\n",
        "\n",
        "#For Colab\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/')\n",
        "import functions as fun\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.options.display.float_format = '{:.5f}'.format\n",
        "df = import_housing_data('http://www.ishelp.info/data/housing_full.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RgjNTb32Ilq-",
        "outputId": "814903b2-1762-469a-8afb-c047427ef957"
      },
      "outputs": [],
      "source": [
        "fun.unistats(df)\n",
        "\n",
        "# Interpreting\n",
        "    # Numeric \n",
        "        # Skewness above 1 or -1 (positive is right skewed, negative is left skewed)\n",
        "    # Categorical\n",
        "        # High number of Unique values when compared to rest of dataframe (ex. Neighborhood )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3VvcDCZOIlrA",
        "outputId": "f0a2bec0-5b2f-4e53-93a1-a6cf8be2ef24"
      },
      "outputs": [],
      "source": [
        "\n",
        "fun.bivstats(df, 'SalePrice')\n",
        "\n",
        "\n",
        "\n",
        "# Bivariate \n",
        "    # Num-Num: Correlation R\n",
        "    # Num-Cat: one-way ANOVA (3+ groups) or T-test (2 groups)\n",
        "    # Cat-Cat: Chi-square\n",
        "\n",
        "\n",
        "\n",
        "# Importance in terms of Correlation\n",
        "# it makes sense that as corr gets smaller, p value (likelihood that what we found is due to chance) gets higher. He cares more about effect size than p value\n",
        "# F\n",
        "    # look at what has the largest effect size relative to this dataframe\n",
        "# r\n",
        "    # first look what numbers are closest to one\n",
        "    # then go look at skewness above to see if there are any issues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MLMTMXIQIlrC",
        "outputId": "8b414cec-3f05-4e6e-9987-2f9e8c539d14"
      },
      "outputs": [],
      "source": [
        "#scatter plot vizzes\n",
        "\n",
        "fun.biv_viz(df, 'SalePrice')\n",
        "\n",
        "# Things to look for \n",
        "    # Scatter Plots (num-num)\n",
        "        # are individual variables normally distributed? Skew and Kurt (you can also look at histograms on sides of jointplot)\n",
        "        # Heteroscadicisity - the spread of the dots is evenly distributed throughout all values of X (in this chart, it is not)\n",
        "            # White Test - low pvalue means YES heteroscadiscity issues\n",
        "            # BP Test - low pvalue means YES heteroscadiscity issues\n",
        "        # Outliers - do you have a dot or two that is FAR away from most other dots? (so far we haven't learned this)\n",
        "    # Bar Charts (Anova - cat/num)\n",
        "        # which of the bars are the highest, and have the smallest stDev (lil tick)\n",
        "\n",
        "#there was a part in the video (around 18:50 in Python Practice: Housing Prices: Bivariate Vizualiations) where he was talking about how we need to come up with a way to keep track of all the ones (low p in white and BP)(skew and kurt over 1) that have issues but I have not idea how lol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLzfs9FWNjqV"
      },
      "outputs": [],
      "source": [
        "def mlr_prepare(df) :\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  from sklearn import preprocessing\n",
        "\n",
        "\n",
        "  #creates dummy variables for you\n",
        "  for col in df:\n",
        "      if not pd.api.types.is_numeric_dtype(df[col]) :\n",
        "        df = df.join(pd.get_dummies(df[col], prefix=col, drop_first=False))\n",
        "\n",
        "  #only has numerics\n",
        "  df = df.select_dtypes(np.number)\n",
        "  #if something is coming out as an list of lists and should be a df, copy the code below this comment to fix that\n",
        "  df_minmax = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(df), columns=df.columns)\n",
        "  return df_minmax\n",
        "\n",
        "\n",
        "df = mlr_prepare(import_housing_data('http://www.ishelp.info/data/housing_full.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2_YHyEgXjdi"
      },
      "outputs": [],
      "source": [
        "#run the MLR\n",
        "def mlr(df, label) :\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import statsmodels.api as sm\n",
        "\n",
        "  y = df[label]\n",
        "  X = df.drop(columns=[label]).assign(const=1)\n",
        "\n",
        "  results = sm.OLS(y, X).fit()\n",
        "  return results\n",
        "\n",
        "\n",
        "results = mlr(df, 'SalePrice')\n",
        "results.summary()\n",
        "\n",
        "#Things to look at\n",
        "# R squared\n",
        "# Adj R Squared - if it is close to R, that means we have a lot of variables that aren't doing us a whole lot of good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "PD-SOn1z7dOX",
        "outputId": "56372de8-e83a-487f-de41-cf79b74c4836"
      },
      "outputs": [],
      "source": [
        "def mlr_feature_df(results):\n",
        "  df_features = pd.DataFrame({'coef':results.params, 't':abs(results.tvalues), 'p':results.pvalues})\n",
        "  df_features.drop(labels=['const'], inplace=True)\n",
        "  df_features.sort_values(by=['t','p'])\n",
        "  return df_features\n",
        "mlr_feature_df(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def vif(df):\n",
        "  # initialize dictionaries\n",
        "  vif_dict, tolerance_dict = {}, {}\n",
        "\n",
        "  # form input data for each exogenous variable\n",
        "  for col in df.drop(columns=['const']):\n",
        "    y = df[col]\n",
        "    X = df.drop(columns=[col])\n",
        "    \n",
        "    # extract r-squared from the fit\n",
        "    r_squared = LinearRegression().fit(X, y).score(X, y)\n",
        "\n",
        "    # calculate VIF\n",
        "    if r_squared < 1: # Prevent division by zero runtime error\n",
        "      vif = 1/(1 - r_squared) \n",
        "    else:\n",
        "      vif = 1\n",
        "    vif_dict[col] = vif\n",
        "\n",
        "    # calculate tolerance\n",
        "    tolerance = 1 - r_squared\n",
        "    tolerance_dict[col] = tolerance\n",
        "\n",
        "  return pd.DataFrame({'VIF': vif_dict, 'Tolerance': tolerance_dict}).sort_values(by=['VIF'], ascending=False)\n",
        "\n",
        "vif(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clmxMJRTVLDI"
      },
      "outputs": [],
      "source": [
        "def mlr_fit(results, actual, roundto=10): #Calculate fit statistics, create a record entry for the modeling results table\n",
        "  import numpy as np\n",
        "\n",
        "  df_features = mlr_feature_df(results) #Generate feature table taht allows sorting coef labels based on t and p\n",
        "  residuals = np.array(actual) - np.array(results.fittedvalues)\n",
        "  rmse = np.sqrt(sum((residuals**2))/len(actual))\n",
        "  mae = np.mean(abs(np.array(actual) - np.array(results.fittedvalues)))\n",
        "  fit_stats = [round(results.rsquared, roundto), round(results.rsquared_adj, roundto), round(results.rsquared - results.rsquared_adj, roundto), round(rmse, roundto), round(mae, roundto), [df_features.index.values]]\n",
        "  return fit_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozaO5pC0ZMzX"
      },
      "outputs": [],
      "source": [
        "def mlr_step(df, label, min=2): #Control mlr and mlr_fit by removing a certain criterion of feature one at a time\n",
        "\n",
        "  #create the empty model results table\n",
        "  df_models = pd.DataFrame(columns=['R2', 'R2a', 'diff', 'rmse', 'MAE', 'features'])\n",
        "\n",
        "  #prepare the data by generating dummies and scaling\n",
        "  df = mlr_prepare(df)\n",
        "\n",
        "  #run the first model with all features\n",
        "  results = mlr(df, label)\n",
        "\n",
        "  #generate the fit statistics for the og model\n",
        "  df_models.loc[str(len(results.params))] = mlr_fit(results, df[label], 10)\n",
        "\n",
        "  # Generate feature table that allows sorting coef labels based on t and p \n",
        "  df_features = mlr_feature_df(results) \n",
        "\n",
        "  # Step through a series of reduced models until you \n",
        "  while len(results.params) >= min:                 # Keep looping as long as there are at least a minimum number of features left \n",
        "    df = df.drop(columns=[df_features.index[0]])    # Drop the least effective feature \n",
        "    results = mlr(df, label)                        # Re-run the next MLR \n",
        "    df_features = mlr_feature_df(results)           # Re-generate the features summary table \n",
        "    df_models.loc[len(results.params)] = mlr_fit(results, df[label], 10) \n",
        "\n",
        "  # Save the full models table to a CSV \n",
        "  df_models.to_csv('/content/drive/My Drive/Colab Notebooks/' + label + '.csv') \n",
        "\n",
        "  # Return to display a shortened version without feature list \n",
        "  df_models.drop(columns=['features'], inplace=True) \n",
        "\n",
        "  return df_models \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "CvAlZmzpaokU",
        "outputId": "cabfc5d9-08d4-44ce-a618-5853a4b34e28"
      },
      "outputs": [],
      "source": [
        "df_models = mlr_step(import_housing_data('http://www.ishelp.info/data/housing_full.csv'), 'SalePrice')\n",
        "df_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mlr_prepare(df) :\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  from sklearn import preprocessing\n",
        "\n",
        "\n",
        "  #creates dummy variables for you\n",
        "  for col in df:\n",
        "      if not pd.api.types.is_numeric_dtype(df[col]) :\n",
        "        df = df.join(pd.get_dummies(df[col], prefix=col, drop_first=False))\n",
        "\n",
        "  #only has numerics\n",
        "  df = df.select_dtypes(np.number)\n",
        "  #if something is coming out as an list of lists and should be a df, copy the code below this comment to fix that\n",
        "  df_minmax = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(df), columns=df.columns)\n",
        "  return df_minmax\n",
        "\n",
        "\n",
        "\n",
        "#run the MLR\n",
        "def mlr(df, label) :\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "\n",
        "  y = df_minmax[label]\n",
        "  X = df_minmax.drop(columns=[label, 'Utilities_AllPub']).assign(const=1)\n",
        "\n",
        "  results = sm.OLS(y, X).fit()\n",
        "  return results\n",
        "results = mlr(df, 'SalePrice')\n",
        "\n",
        "#Things to look at\n",
        "# R squared\n",
        "# Adj R Squared - if it is close to R, that means we have a lot of variables that aren't doing us a whole lot of good\n",
        "\n",
        "\n",
        "def mlr_feature_df(results):\n",
        "  df_features = pd.DataFrame({'coef':results.params, 't':abs(results.tvalues), 'p':results.pvalues})\n",
        "  df_features.drop(labels=['const'], inplace=True)\n",
        "  df_features.sort_values(by=['t','p'])\n",
        "  return df_features\n",
        "mlr_feature_df(results)\n",
        "\n",
        "\n",
        "\n",
        "def mlr_fit(results, actual, roundto=10): #Calculate fit statistics, create a record entry for the modeling results table\n",
        "  import numpy as np\n",
        "\n",
        "  df_features = mlr_feature_df(results) #Generate feature table taht allows sorting coef labels based on t and p\n",
        "  residuals = np.array(actual) - np.array(results.fittedvalues)\n",
        "  rmse = np.sqrt(sum((residuals**2))/len(actual))\n",
        "  mae = np.mean(abs(np.array(actual) - np.array(results.fittedvalues)))\n",
        "  fit_stats = [round(results.rsquared, roundto), round(results.rsquared_adj, roundto), round(results.rsquared - results.rsquared_adj, roundto), round(rmse, roundto), round(mae, roundto), [df_features.index.values]]\n",
        "  return fit_stats\n",
        "\n",
        "\n",
        "def mlr_step(df, label, min=2): #Control mlr and mlr_fit by removing a certain criterion of feature one at a time\n",
        "\n",
        "  #create the empty model results table\n",
        "  df_models = pd.DataFrame(columns=['R2', 'R2a', 'diff', 'rmse', 'MAE', 'features'])\n",
        "\n",
        "  #prepare the data by generating dummies and scaling\n",
        "  df = mlr_prepare(df)\n",
        "\n",
        "  #run the first model with all features\n",
        "  results = mlr(df, label)\n",
        "\n",
        "  #generate the fit statistics for the og model\n",
        "  df_models.loc[str(len(results.params))] = mlr_fit(results, df[label], 10)\n",
        "\n",
        "  # Generate feature table that allows sorting coef labels based on t and p \n",
        "  df_features = mlr_feature_df(results) \n",
        "\n",
        "  # Step through a series of reduced models until you \n",
        "  while len(results.params) >= min:                 # Keep looping as long as there are at least a minimum number of features left \n",
        "    df = df.drop(columns=[df_features.index[0]])    # Drop the least effective feature \n",
        "    results = mlr(df, label)                        # Re-run the next MLR \n",
        "    df_features = mlr_feature_df(results)           # Re-generate the features summary table \n",
        "    df_models.loc[len(results.params)] = mlr_fit(results, df[label], 10) \n",
        "\n",
        "  # Save the full models table to a CSV \n",
        "  df_models.to_csv('/content/drive/My Drive/Colab Notebooks/' + label + '.csv') \n",
        "\n",
        "  # Return to display a shortened version without feature list \n",
        "  df_models.drop(columns=['features'], inplace=True) \n",
        "\n",
        "  return df_models \n",
        "\n",
        "\n",
        "\n",
        "df_models = mlr_step(import_housing_data('http://www.ishelp.info/data/housing_full.csv'), 'SalePrice')\n",
        "df_models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "A53zWUpNbSH3",
        "outputId": "103e826b-6374-41c2-d533-d399466a4ea1"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "fig, ax = plt.subplots(figsize=(15,6))\n",
        "ax = sns.lineplot(df_models.index, df_models.R2)\n",
        "ax = sns.lineplot(df_models.index, df_models.R2a)\n",
        "ax.set(ylin=(.88, .921));\n",
        "\n",
        "# the correct number of variables you should have is from where the plot evens out, and anything to the left of that\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnQYRME2ex_m"
      },
      "source": [
        "I made it up to 7:15 in the 5th video, but my code from 2 blocks up is taking 5ever to run. It might be an issue with my code 3 blocks up but I don't see what it could be. From stack overflow it looks like it might be an issue with the dropna axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f5J83Dofuao"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "20.11 Complete Practice.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "9d67350b9e01b4a1219347fcb6eec091e85586fd2bdf9a4f55866424391477a0"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('myenv': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
